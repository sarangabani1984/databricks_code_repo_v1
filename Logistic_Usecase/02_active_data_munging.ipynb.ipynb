{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4737506e-0ac5-42b9-a8c2-da828bc4d09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Logistics_Data_Engineering\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18428e4-a2c5-4d48-b918-b5489222b0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Volume paths (raw source data)\n",
    "source_path_v1 = \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1\"\n",
    "source_path_v2 = \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e688a626-8d4b-4bcb-8d5f-40ec2845df3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Combining Data + Schema Merging (Structuring) <br>\n",
    "Read both files without enforcing schema <br>\n",
    "Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source <br>\n",
    "Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1725f6d4-9579-45e0-b0d9-dea220cf0015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Read both source files (NO schema enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49e110d-8aa0-48dc-b19e-db508594e57b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768461933157}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 1: Read raw source files without enforcing schema\n",
    "# --------------------------------------------------\n",
    "# All columns are read as STRING to avoid early data loss\n",
    "\n",
    "df_system1 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(source_path_v1)\n",
    ")\n",
    "\n",
    "df_system2 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(source_path_v2)\n",
    ")\n",
    "\n",
    "display(df_system1)\n",
    "display(df_system2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a95b8a3-da60-4208-b53f-84fa01d06eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ 2. Add data_source column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277347fd-9532-4def-a7d8-5ab763e3af1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 2: Add data_source column\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_system1 = df_system1.withColumn(\"data_source\", lit(\"system1\"))\n",
    "df_system2 = df_system2.withColumn(\"data_source\", lit(\"system2\"))\n",
    "\n",
    "display(df_system1)\n",
    "display(df_system2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aca8dcd-0c21-4334-b02f-f9b66822b62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Add missing columns to system1 to match canonical schema\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_system1_aligned = (\n",
    "    df_system1\n",
    "    .withColumn(\"hub_location\", lit(None))\n",
    "    .withColumn(\"vehicle_type\", lit(None))\n",
    ")\n",
    "\n",
    "display(df_system1_aligned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3043c257-1af5-48da-8869-d7d814c9c8f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 3b: Align system2 schema\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_system2_aligned = (\n",
    "    df_system2\n",
    "    .select(\n",
    "        \"shipment_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"age\",\n",
    "        \"role\",\n",
    "        \"hub_location\",\n",
    "        \"vehicle_type\",\n",
    "        \"data_source\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2730ea8c-2364-46c9-bbee-c41a37ddda78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ 4. Union both datasets into ONE canonical DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc82686-c96f-4421-b48b-0cfc647f8c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 4: Combine both systems into a single dataset\n",
    "# --------------------------------------------------\n",
    "\n",
    "combined_df = df_system1_aligned.unionByName(df_system2_aligned)\n",
    "\n",
    "display(combined_df)\n",
    "combined_df.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_active_data_munging.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
