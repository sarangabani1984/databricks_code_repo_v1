{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4737506e-0ac5-42b9-a8c2-da828bc4d09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Logistics_Data_Engineering\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18428e4-a2c5-4d48-b918-b5489222b0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Volume paths (raw source data)\n",
    "source_path_v1 = \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1\"\n",
    "source_path_v2 = \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e688a626-8d4b-4bcb-8d5f-40ec2845df3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Combining Data + Schema Merging (Structuring) <br>\n",
    "Read both files without enforcing schema <br>\n",
    "Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source <br>\n",
    "Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1725f6d4-9579-45e0-b0d9-dea220cf0015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Read both source files (NO schema enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49e110d-8aa0-48dc-b19e-db508594e57b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768461933157}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 1: Read raw source files without enforcing schema\n",
    "# --------------------------------------------------\n",
    "# All columns are read as STRING to avoid early data loss\n",
    "\n",
    "df_system1 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(source_path_v1)\n",
    ")\n",
    "\n",
    "df_system2 = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(source_path_v2)\n",
    ")\n",
    "\n",
    "display(df_system1)\n",
    "display(df_system2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a95b8a3-da60-4208-b53f-84fa01d06eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ 2. Add data_source column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277347fd-9532-4def-a7d8-5ab763e3af1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 2: Add data_source column\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_system1 = df_system1.withColumn(\"data_source\", lit(\"system1\"))\n",
    "df_system2 = df_system2.withColumn(\"data_source\", lit(\"system2\"))\n",
    "\n",
    "display(df_system1)\n",
    "display(df_system2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aca8dcd-0c21-4334-b02f-f9b66822b62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Add missing columns to system1 to match canonical schema\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_system1_aligned = (\n",
    "    df_system1\n",
    "    .withColumn(\"hub_location\", lit(None))\n",
    "    .withColumn(\"vehicle_type\", lit(None))\n",
    ")\n",
    "\n",
    "display(df_system1_aligned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3043c257-1af5-48da-8869-d7d814c9c8f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 3b: Align system2 schema\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_system2_aligned = (\n",
    "    df_system2\n",
    "    .select(\n",
    "        \"shipment_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"age\",\n",
    "        \"role\",\n",
    "        \"hub_location\",\n",
    "        \"vehicle_type\",\n",
    "        \"data_source\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2730ea8c-2364-46c9-bbee-c41a37ddda78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ 4. Union both datasets into ONE canonical DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc82686-c96f-4421-b48b-0cfc647f8c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 4: Combine both systems into a single dataset\n",
    "# --------------------------------------------------\n",
    "\n",
    "combined_df = df_system1_aligned.unionByName(df_system2_aligned)\n",
    "\n",
    "display(combined_df)\n",
    "combined_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b14ee2-4b1d-4d34-bb7b-ec6e0ba6edc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0391ea2-914a-4491-b937-0f138b67610e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f0e47e-b33d-4aed-a67f-64111ea14f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Mandatory Column Check\n",
    "# shipment_id and role are mandatory business fields\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Records failing mandatory column check\n",
    "rejected_mandatory_df = combined_df.filter(\n",
    "    col(\"shipment_id\").isNull() | col(\"role\").isNull()\n",
    ")\n",
    "\n",
    "# Records passing mandatory column check\n",
    "valid_mandatory_df = combined_df.dropna(\n",
    "    subset=[\"shipment_id\", \"role\"]\n",
    ")\n",
    "\n",
    "# Audit counts\n",
    "print(\"Total records before check:\", combined_df.count())\n",
    "print(\"Records rejected (mandatory fields missing):\", rejected_mandatory_df.count())\n",
    "print(\"Records retained after check:\", valid_mandatory_df.count())\n",
    "\n",
    "# Display for validation\n",
    "display(rejected_mandatory_df)\n",
    "display(valid_mandatory_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f240b19d-c2c6-40ef-ad4e-9be76f6201c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e4ded35f-12a1-46ad-8236-dab2560ba685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Name Completeness Rule\n",
    "# Drop records where BOTH first_name AND last_name are NULL\n",
    "# --------------------------------------------------\n",
    "\n",
    "rejected_name_df = valid_mandatory_df.filter(\n",
    "    col(\"first_name\").isNull() & col(\"last_name\").isNull()\n",
    ")\n",
    "\n",
    "valid_name_df = valid_mandatory_df.filter(\n",
    "    ~(col(\"first_name\").isNull() & col(\"last_name\").isNull())\n",
    ")\n",
    "\n",
    "print(\"Records before name completeness check:\", valid_mandatory_df.count())\n",
    "print(\"Records rejected (both names missing):\", rejected_name_df.count())\n",
    "print(\"Records retained after name completeness check:\", valid_name_df.count())\n",
    "\n",
    "display(rejected_name_df)\n",
    "display(valid_name_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1adaba6-2aef-4b26-a775-b66aff974699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ Join Readiness Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0013fd5f-5105-4a2a-9d8b-c1d1d723482e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Join Readiness Rule\n",
    "# Drop records where shipment_id is NULL\n",
    "# --------------------------------------------------\n",
    "\n",
    "rejected_join_df = valid_name_df.filter(\n",
    "    col(\"shipment_id\").isNull()\n",
    ")\n",
    "\n",
    "join_ready_df = valid_name_df.filter(\n",
    "    col(\"shipment_id\").isNotNull()\n",
    ")\n",
    "\n",
    "print(\"Records before join readiness check:\", valid_name_df.count())\n",
    "print(\"Records rejected (shipment_id is NULL):\", rejected_join_df.count())\n",
    "print(\"Records ready for join:\", join_ready_df.count())\n",
    "\n",
    "display(rejected_join_df)\n",
    "display(join_ready_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a78a0b-c55b-47bf-b835-3d9925172611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_ready_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e30c96f-6e1a-4502-8c22-1c28c87683a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, expr, lit\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Scrubbing Rule 4: Age Defaulting\n",
    "# --------------------------------------------------\n",
    "# Business Rule:\n",
    "# - If `age` is NULL or non-numeric, replace with -1\n",
    "# - Valid numeric values are preserved\n",
    "# --------------------------------------------------\n",
    "\n",
    "join_ready_df = (\n",
    "    join_ready_df\n",
    "        .withColumn(\n",
    "            \"age\",\n",
    "            coalesce(expr(\"try_cast(age as bigint)\"), lit(-1))\n",
    "        )\n",
    ")\n",
    "\n",
    "# Verify the result\n",
    "display(join_ready_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_active_data_munging.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
