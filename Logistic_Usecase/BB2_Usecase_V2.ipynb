{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de418ac9-da9d-4a40-8362-c19638bc92e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd26a48-afd9-4032-8923-af2f74dee1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYSPARK EDA - Logistics Data Analysis\n",
    "# ============================================================\n",
    "\n",
    "# 1. Read CSV and create DataFrame with renamed columns\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,           # First row is column names\n",
    "    inferSchema=True       # Automatically detect data types\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Display the actual data (first 20 rows)\n",
    "# ============================================================\n",
    "display(df)\n",
    "\n",
    "# ============================================================\n",
    "# 3. View the Schema (column names and their data types)\n",
    "# ============================================================\n",
    "print(\"SCHEMA DETAILS:\")\n",
    "df.printSchema()\n",
    "\n",
    "# ============================================================\n",
    "# 4. Show Data Types of each column\n",
    "# ============================================================\n",
    "print(\"\\nDATA TYPES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Show all column names\n",
    "# ============================================================\n",
    "print(\"\\nCOLUMN NAMES:\")\n",
    "print(df.columns)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Check for Duplicate Records (by Ship_id)\n",
    "# ============================================================\n",
    "print(\"\\nDUPLICATE CHECK - Ship_ids appearing more than once:\")\n",
    "df.groupBy(\"Ship_id\").count().filter(\"count > 1\").show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Show Summary Statistics (count, mean, stddev, min, max)\n",
    "# ============================================================\n",
    "print(\"\\nDATAFRAME SUMMARY:\")\n",
    "df.summary().show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BONUS: Additional useful checks\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Check total rows\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "\n",
    "# Check null values in each column\n",
    "print(f\"\\nNull Value Count by Column:\")\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce9dc62-432d-411d-8c75-82a8a96e65e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. LOAD BOTH FILES\n",
    "# ============================================================\n",
    "\n",
    "# Load logistics_source1 (master_v1)\n",
    "master_v1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# Load logistics_source2 (master_v2)\n",
    "master_v2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\",\"location\",\"vhicle_type\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MASTER_V1 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v1.show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MASTER_V2 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c6cfdb-4abc-469c-a938-7a54d5bbc267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# FIND COMMON SHIPMENT IDs BETWEEN TWO DATASETS\n",
    "# ============================================================\n",
    "\n",
    "# Extract unique Ship_ids from both datasets\n",
    "v1_ids = master_v1.select(\"Ship_id\").distinct()\n",
    "v2_ids = master_v2.select(\"Ship_id\").distinct()\n",
    "\n",
    "# Find common Ship_ids (intersection - present in both)\n",
    "common_ids = v1_ids.intersect(v2_ids)\n",
    "\n",
    "# Display results\n",
    "print(f\"Common Shipment IDs: {common_ids.count()}\")\n",
    "common_ids.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09359f1-b571-4673-a2c3-18ab72781bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "non_numeric=master_v1.filter(col(\"Ship_id\").rlike(\"^[0-9]\"))\n",
    "non_numeric.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d6d030e-0a57-4e9b-ba40-c264c32fae4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you want the COUNT (interview-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2eafa4-f008-446c-b359-54d6cf02d71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "non_numeric_count = master_v1.filter(\n",
    "    ~col(\"Ship_id\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "non_numeric_count2 = master_v2.filter(\n",
    "    ~col(\"Ship_id\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "\n",
    "print(\"Non-numeric Ship_id count:\", non_numeric_count)\n",
    "print(\"Non-numeric Ship_id count:\", non_numeric_count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5991e22-67aa-4f5d-9fd5-64ac8e11536f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "age is not an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38f6d57-6fc0-4bcd-bb43-861f9ddfce98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_age_count = master_v1.filter(\n",
    "    ~col(\"age\").cast(\"string\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "print(\"Non-integer age count:\", invalid_age_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c409e0a-d11a-4f1a-91df-3648a258cbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_age_count2 = master_v2.filter(\n",
    "    ~col(\"age\").cast(\"string\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "print(\"Non-integer age count:\", invalid_age_count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086ef94a-caae-467b-b45a-27986cb25297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff228194-9fbb-4fa9-a83c-771847c904fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4def3372-ff93-41c6-ab42-80ebeeb15e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA INTEGRATION - Combining Two Sources with Schema Alignment\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Read both files without enforcing schema\n",
    "# ============================================================\n",
    "\n",
    "df_s1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "df_s2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"SOURCE 1 - Columns:\")\n",
    "print(df_s1.columns)\n",
    "df_s1.show()\n",
    "\n",
    "print(\"\\nSOURCE 2 - Columns:\")\n",
    "print(df_s2.columns)\n",
    "df_s2.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Align columns to canonical schema\n",
    "# ============================================================\n",
    "# Both sources already have: shipment_id, first_name, last_name, age, role\n",
    "# Need to add: hub_location, vehicle_type (if missing)\n",
    "\n",
    "df_s1_aligned = df_s1.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"role\"), \n",
    ")\n",
    "\n",
    "df_s2_aligned = df_s2.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"role\"),\n",
    "    col(\"hub_location\"),\n",
    "    col(\"vehicle_type\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Add data_source column\n",
    "# ============================================================\n",
    "\n",
    "df_s1_with_source = df_s1_aligned.withColumn(\"data_source\", lit(\"system1\"))\n",
    "df_s2_with_source = df_s2_aligned.withColumn(\"data_source\", lit(\"system2\"))\n",
    "\n",
    "print(\"\\nSOURCE 1 - With data_source:\")\n",
    "df_s1_with_source.show()\n",
    "\n",
    "print(\"\\nSOURCE 2 - With data_source:\")\n",
    "df_s2_with_source.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Merge both dataframes\n",
    "# ============================================================\n",
    "\n",
    "df_merged = df_s1_with_source.unionByName(\n",
    "    df_s2_with_source,\n",
    "    allowMissingColumns=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGED DATA - Final Schema\")\n",
    "print(\"=\"*60)\n",
    "display(df_merged)\n",
    "\n",
    "print(f\"\\nTotal rows: {df_merged.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e19f8ce-33aa-4ffe-ad22-65276322a968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de43f96e-47be-4929-ac15-df61442e424a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean data by dropping NULL records\n",
    "df_cleaned = df_merged.dropna(\n",
    "    how=\"any\",\n",
    "    subset=[\"shipment_id\", \"role\"]\n",
    ").dropna(\n",
    "    how=\"all\",\n",
    "    subset=[\"first_name\", \"last_name\"]\n",
    ")\n",
    "\n",
    "print(f\"Cleaned rows: {df_cleaned.count()}\")\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed8f9b-6294-45fe-a7aa-146e541e886a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# All scrubbing steps in one operation\n",
    "df_scrubbed = df_cleaned.fillna({\n",
    "    \"age\": -1,\n",
    "    \"vehicle_type\": \"UNKNOWN\"\n",
    "}).withColumn(\n",
    "    \"age\",\n",
    "    when((col(\"age\") == \"ten\") | (col(\"age\") == \"\"), -1).otherwise(col(\"age\"))\n",
    ").withColumn(\n",
    "    \"vehicle_type\",\n",
    "    when(col(\"vehicle_type\") == \"Truck\", \"LMV\")\n",
    "    .when(col(\"vehicle_type\") == \"Bike\", \"TwoWheeler\")\n",
    "    .otherwise(col(\"vehicle_type\"))\n",
    ")\n",
    "\n",
    "display(df_scrubbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77b3cb1f-2873-41ff-8426-025699ac21e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46c91acf-a18d-4b4e-ad87-fb801ab6ab72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ed63ee-7d86-4700-96e0-85525c8c8fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SHIPMENT DETAILS DATA - Create DataFrame from JSON\n",
    "# ============================================================\n",
    "\n",
    "# Read multi-line JSON file\n",
    "df_shipment = spark.read.json(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_shipment_detail_3000.json\",\n",
    "    multiLine=True\n",
    ")\n",
    "\n",
    "# Display data and schema\n",
    "print(\"SHIPMENT DETAILS DATA:\")\n",
    "display(df_shipment)\n",
    "\n",
    "print(\"\\nSCHEMA:\")\n",
    "df_shipment.printSchema()\n",
    "\n",
    "print(f\"\\nTotal records: {df_shipment.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f5ec9e-afb4-437a-af1b-99686657b948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SHIPMENT DETAILS ENRICHMENT - Add Metadata Columns\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "# ============================================================\n",
    "# Add enrichment columns:\n",
    "# 1. domain: \"Logistics\"\n",
    "# 2. ingestion_timestamp: Current timestamp when data was loaded\n",
    "# 3. is_expedited: False (default value)\n",
    "# ============================================================\n",
    "\n",
    "df_shipment_enriched = df_shipment.withColumn(\n",
    "    \"domain\",\n",
    "    lit(\"Logistics\")\n",
    ").withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    current_timestamp()\n",
    ").withColumn(\n",
    "    \"is_expedited\",\n",
    "    lit(False)\n",
    ")\n",
    "\n",
    "print(\"ENRICHED SHIPMENT DATA:\")\n",
    "display(df_shipment_enriched)\n",
    "\n",
    "# ============================================================\n",
    "# Verify enriched columns\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA - With Enrichment Columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_enriched.printSchema()\n",
    "\n",
    "print(f\"\\nTotal records: {df_shipment_enriched.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25776626-585b-4dd6-804e-ec30363f3dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COLUMN UNIFORMITY - Standardize Text Case Across Sources\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, lower, upper, initcap\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 1: df_scrubbed (Merged logistics_source1 & source2)\n",
    "# ============================================================\n",
    "\n",
    "# Convert role to LOWERCASE\n",
    "# Convert hub_location to INITCAP (First Letter Capitalized)\n",
    "\n",
    "df_scrubbed_uniform = df_scrubbed.withColumn(\n",
    "    \"role\",\n",
    "    lower(col(\"role\"))\n",
    ").withColumn(\n",
    "    \"hub_location\",\n",
    "    initcap(col(\"hub_location\"))\n",
    ")\n",
    "\n",
    "print(\"STANDARDIZED DATA - df_scrubbed:\")\n",
    "print(\"=\"*60)\n",
    "df_scrubbed_uniform.show()\n",
    "\n",
    "print(\"\\nUnique ROLE values (lowercase):\")\n",
    "df_scrubbed_uniform.select(\"role\").distinct().show()\n",
    "\n",
    "print(\"\\nUnique HUB_LOCATION values (initcap):\")\n",
    "df_scrubbed_uniform.select(\"hub_location\").distinct().show()\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 2: df_shipment_enriched (JSON shipment details)\n",
    "# ============================================================\n",
    "\n",
    "# Convert vehicle_type to UPPERCASE\n",
    "\n",
    "df_shipment_uniform = df_shipment_enriched.withColumn(\n",
    "    \"vehicle_type\",\n",
    "    upper(col(\"vehicle_type\"))\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STANDARDIZED DATA - df_shipment_enriched:\")\n",
    "print(\"=\"*60)\n",
    "df_shipment_uniform.show()\n",
    "\n",
    "print(\"\\nUnique VEHICLE_TYPE values (uppercase):\")\n",
    "df_shipment_uniform.select(\"vehicle_type\").distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e20526-6e3a-416a-827a-9d88796aaa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format Standardization:<BR>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<BR>\n",
    "Convert shipment_date to yyyy-MM-dd<BR>\n",
    "Ensure shipment_cost has 2 decimal precision<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2a3b5f-b00f-484f-8678-50c307244a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FORMAT STANDARDIZATION - Date & Currency Formatting\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, try_to_date, round as spark_round\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE: df_shipment_enriched (logistics_shipment_detail_3000.json)\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Convert shipment_date to yyyy-MM-dd format\n",
    "# Use try_to_date() to safely handle invalid dates\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Ensure shipment_cost has 2 decimal precision\n",
    "# Cast to double and round to 2 decimal places\n",
    "# ============================================================\n",
    "\n",
    "df_shipment_formatted = df_shipment_enriched.withColumn(\n",
    "    \"shipment_date\",\n",
    "    try_to_date(col(\"shipment_date\"), \"dd-MM-yy\")\n",
    ").withColumn(\n",
    "    \"shipment_cost\",\n",
    "    spark_round(col(\"shipment_cost\").cast(\"double\"), 2)\n",
    ")\n",
    "\n",
    "print(\"FORMATTED DATA - Date & Currency Standardized:\")\n",
    "print(\"=\"*60)\n",
    "display(df_shipment_formatted)\n",
    "\n",
    "# ============================================================\n",
    "# VERIFICATION - Check formatting quality\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHIPMENT DATE VALUES (yyyy-MM-dd format):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_formatted.select(\"shipment_date\").show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHIPMENT COST VALUES (2 decimal precision):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_formatted.select(\"shipment_cost\").show(5)\n",
    "\n",
    "# ============================================================\n",
    "# DATE PARSING QUALITY CHECK\n",
    "# ============================================================\n",
    "\n",
    "total_rows = df_shipment_formatted.count()\n",
    "null_dates = df_shipment_formatted.filter(col(\"shipment_date\").isNull()).count()\n",
    "valid_dates = total_rows - null_dates\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {total_rows}\")\n",
    "print(f\"Valid dates: {valid_dates}\")\n",
    "print(f\"Invalid dates (NULL): {null_dates}\")\n",
    "\n",
    "# ============================================================\n",
    "# SCHEMA - Verify data types\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA - Data Types After Formatting\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_formatted.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase_V2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
