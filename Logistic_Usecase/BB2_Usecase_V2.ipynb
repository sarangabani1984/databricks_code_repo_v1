{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de418ac9-da9d-4a40-8362-c19638bc92e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd26a48-afd9-4032-8923-af2f74dee1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYSPARK EDA - Logistics Data Analysis\n",
    "# ============================================================\n",
    "\n",
    "# 1. Read CSV and create DataFrame with renamed columns\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,           # First row is column names\n",
    "    inferSchema=True       # Automatically detect data types\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Display the actual data (first 20 rows)\n",
    "# ============================================================\n",
    "display(df)\n",
    "\n",
    "# ============================================================\n",
    "# 3. View the Schema (column names and their data types)\n",
    "# ============================================================\n",
    "print(\"SCHEMA DETAILS:\")\n",
    "df.printSchema()\n",
    "\n",
    "# ============================================================\n",
    "# 4. Show Data Types of each column\n",
    "# ============================================================\n",
    "print(\"\\nDATA TYPES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Show all column names\n",
    "# ============================================================\n",
    "print(\"\\nCOLUMN NAMES:\")\n",
    "print(df.columns)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Check for Duplicate Records (by Ship_id)\n",
    "# ============================================================\n",
    "print(\"\\nDUPLICATE CHECK - Ship_ids appearing more than once:\")\n",
    "df.groupBy(\"Ship_id\").count().filter(\"count > 1\").show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Show Summary Statistics (count, mean, stddev, min, max)\n",
    "# ============================================================\n",
    "print(\"\\nDATAFRAME SUMMARY:\")\n",
    "df.summary().show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BONUS: Additional useful checks\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Check total rows\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "\n",
    "# Check null values in each column\n",
    "print(f\"\\nNull Value Count by Column:\")\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce9dc62-432d-411d-8c75-82a8a96e65e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. LOAD BOTH FILES\n",
    "# ============================================================\n",
    "\n",
    "# Load logistics_source1 (master_v1)\n",
    "master_v1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# Load logistics_source2 (master_v2)\n",
    "master_v2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\",\"location\",\"vhicle_type\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MASTER_V1 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v1.show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MASTER_V2 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c6cfdb-4abc-469c-a938-7a54d5bbc267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# FIND COMMON SHIPMENT IDs BETWEEN TWO DATASETS\n",
    "# ============================================================\n",
    "\n",
    "# Extract unique Ship_ids from both datasets\n",
    "v1_ids = master_v1.select(\"Ship_id\").distinct()\n",
    "v2_ids = master_v2.select(\"Ship_id\").distinct()\n",
    "\n",
    "# Find common Ship_ids (intersection - present in both)\n",
    "common_ids = v1_ids.intersect(v2_ids)\n",
    "\n",
    "# Display results\n",
    "print(f\"Common Shipment IDs: {common_ids.count()}\")\n",
    "common_ids.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase_V2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
