{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de418ac9-da9d-4a40-8362-c19638bc92e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd26a48-afd9-4032-8923-af2f74dee1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYSPARK EDA - Logistics Data Analysis\n",
    "# ============================================================\n",
    "\n",
    "# 1. Read CSV and create DataFrame with renamed columns\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,           # First row is column names\n",
    "    inferSchema=True       # Automatically detect data types\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Display the actual data (first 20 rows)\n",
    "# ============================================================\n",
    "display(df)\n",
    "\n",
    "# ============================================================\n",
    "# 3. View the Schema (column names and their data types)\n",
    "# ============================================================\n",
    "print(\"SCHEMA DETAILS:\")\n",
    "df.printSchema()\n",
    "\n",
    "# ============================================================\n",
    "# 4. Show Data Types of each column\n",
    "# ============================================================\n",
    "print(\"\\nDATA TYPES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Show all column names\n",
    "# ============================================================\n",
    "print(\"\\nCOLUMN NAMES:\")\n",
    "print(df.columns)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Check for Duplicate Records (by Ship_id)\n",
    "# ============================================================\n",
    "print(\"\\nDUPLICATE CHECK - Ship_ids appearing more than once:\")\n",
    "df.groupBy(\"Ship_id\").count().filter(\"count > 1\").show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Show Summary Statistics (count, mean, stddev, min, max)\n",
    "# ============================================================\n",
    "print(\"\\nDATAFRAME SUMMARY:\")\n",
    "df.summary().show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BONUS: Additional useful checks\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Check total rows\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "\n",
    "# Check null values in each column\n",
    "print(f\"\\nNull Value Count by Column:\")\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce9dc62-432d-411d-8c75-82a8a96e65e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. LOAD BOTH FILES\n",
    "# ============================================================\n",
    "\n",
    "# Load logistics_source1 (master_v1)\n",
    "master_v1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# Load logistics_source2 (master_v2)\n",
    "master_v2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\",\"location\",\"vhicle_type\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MASTER_V1 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v1.show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MASTER_V2 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c6cfdb-4abc-469c-a938-7a54d5bbc267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# FIND COMMON SHIPMENT IDs BETWEEN TWO DATASETS\n",
    "# ============================================================\n",
    "\n",
    "# Extract unique Ship_ids from both datasets\n",
    "v1_ids = master_v1.select(\"Ship_id\").distinct()\n",
    "v2_ids = master_v2.select(\"Ship_id\").distinct()\n",
    "\n",
    "# Find common Ship_ids (intersection - present in both)\n",
    "common_ids = v1_ids.intersect(v2_ids)\n",
    "\n",
    "# Display results\n",
    "print(f\"Common Shipment IDs: {common_ids.count()}\")\n",
    "common_ids.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09359f1-b571-4673-a2c3-18ab72781bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "non_numeric=master_v1.filter(col(\"Ship_id\").rlike(\"^[0-9]\"))\n",
    "non_numeric.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d6d030e-0a57-4e9b-ba40-c264c32fae4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you want the COUNT (interview-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2eafa4-f008-446c-b359-54d6cf02d71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "non_numeric_count = master_v1.filter(\n",
    "    ~col(\"Ship_id\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "non_numeric_count2 = master_v2.filter(\n",
    "    ~col(\"Ship_id\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "\n",
    "print(\"Non-numeric Ship_id count:\", non_numeric_count)\n",
    "print(\"Non-numeric Ship_id count:\", non_numeric_count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5991e22-67aa-4f5d-9fd5-64ac8e11536f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "age is not an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38f6d57-6fc0-4bcd-bb43-861f9ddfce98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_age_count = master_v1.filter(\n",
    "    ~col(\"age\").cast(\"string\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "print(\"Non-integer age count:\", invalid_age_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c409e0a-d11a-4f1a-91df-3648a258cbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_age_count2 = master_v2.filter(\n",
    "    ~col(\"age\").cast(\"string\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "print(\"Non-integer age count:\", invalid_age_count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "086ef94a-caae-467b-b45a-27986cb25297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff228194-9fbb-4fa9-a83c-771847c904fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4def3372-ff93-41c6-ab42-80ebeeb15e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA INTEGRATION - Combining Two Sources with Schema Alignment\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Read both files without enforcing schema\n",
    "# ============================================================\n",
    "\n",
    "df_s1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "df_s2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"SOURCE 1 - Columns:\")\n",
    "print(df_s1.columns)\n",
    "df_s1.show()\n",
    "\n",
    "print(\"\\nSOURCE 2 - Columns:\")\n",
    "print(df_s2.columns)\n",
    "df_s2.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Align columns to canonical schema\n",
    "# ============================================================\n",
    "# Both sources already have: shipment_id, first_name, last_name, age, role\n",
    "# Need to add: hub_location, vehicle_type (if missing)\n",
    "\n",
    "df_s1_aligned = df_s1.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"role\"), \n",
    ")\n",
    "\n",
    "df_s2_aligned = df_s2.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"role\"),\n",
    "    col(\"hub_location\"),\n",
    "    col(\"vehicle_type\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Add data_source column\n",
    "# ============================================================\n",
    "\n",
    "df_s1_with_source = df_s1_aligned.withColumn(\"data_source\", lit(\"system1\"))\n",
    "df_s2_with_source = df_s2_aligned.withColumn(\"data_source\", lit(\"system2\"))\n",
    "\n",
    "print(\"\\nSOURCE 1 - With data_source:\")\n",
    "df_s1_with_source.show()\n",
    "\n",
    "print(\"\\nSOURCE 2 - With data_source:\")\n",
    "df_s2_with_source.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Merge both dataframes\n",
    "# ============================================================\n",
    "\n",
    "df_merged = df_s1_with_source.unionByName(\n",
    "    df_s2_with_source,\n",
    "    allowMissingColumns=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGED DATA - Final Schema\")\n",
    "print(\"=\"*60)\n",
    "display(df_merged)\n",
    "\n",
    "print(f\"\\nTotal rows: {df_merged.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e19f8ce-33aa-4ffe-ad22-65276322a968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de43f96e-47be-4929-ac15-df61442e424a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean data by dropping NULL records\n",
    "df_cleaned = df_merged.dropna(\n",
    "    how=\"any\",\n",
    "    subset=[\"shipment_id\", \"role\"]\n",
    ").dropna(\n",
    "    how=\"all\",\n",
    "    subset=[\"first_name\", \"last_name\"]\n",
    ")\n",
    "\n",
    "print(f\"Cleaned rows: {df_cleaned.count()}\")\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed8f9b-6294-45fe-a7aa-146e541e886a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# All scrubbing steps in one operation\n",
    "df_scrubbed = df_cleaned.fillna({\n",
    "    \"age\": -1,\n",
    "    \"vehicle_type\": \"UNKNOWN\"\n",
    "}).withColumn(\n",
    "    \"age\",\n",
    "    when((col(\"age\") == \"ten\") | (col(\"age\") == \"\"), -1).otherwise(col(\"age\"))\n",
    ").withColumn(\n",
    "    \"vehicle_type\",\n",
    "    when(col(\"vehicle_type\") == \"Truck\", \"LMV\")\n",
    "    .when(col(\"vehicle_type\") == \"Bike\", \"TwoWheeler\")\n",
    "    .otherwise(col(\"vehicle_type\"))\n",
    ")\n",
    "\n",
    "display(df_scrubbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b3cb1f-2873-41ff-8426-025699ac21e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c91acf-a18d-4b4e-ad87-fb801ab6ab72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ed63ee-7d86-4700-96e0-85525c8c8fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SHIPMENT DETAILS DATA - Create DataFrame from JSON\n",
    "# ============================================================\n",
    "\n",
    "# Read multi-line JSON file\n",
    "df_shipment = spark.read.json(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_shipment_detail_3000.json\",\n",
    "    multiLine=True\n",
    ")\n",
    "\n",
    "# Display data and schema\n",
    "print(\"SHIPMENT DETAILS DATA:\")\n",
    "display(df_shipment)\n",
    "\n",
    "print(\"\\nSCHEMA:\")\n",
    "df_shipment.printSchema()\n",
    "\n",
    "print(f\"\\nTotal records: {df_shipment.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f5ec9e-afb4-437a-af1b-99686657b948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SHIPMENT DETAILS ENRICHMENT - Add Metadata Columns\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "# ============================================================\n",
    "# Add enrichment columns:\n",
    "# 1. domain: \"Logistics\"\n",
    "# 2. ingestion_timestamp: Current timestamp when data was loaded\n",
    "# 3. is_expedited: False (default value)\n",
    "# ============================================================\n",
    "\n",
    "df_shipment_enriched = df_shipment.withColumn(\n",
    "    \"domain\",\n",
    "    lit(\"Logistics\")\n",
    ").withColumn(\n",
    "    \"ingestion_timestamp\",\n",
    "    current_timestamp()\n",
    ").withColumn(\n",
    "    \"is_expedited\",\n",
    "    lit(False)\n",
    ")\n",
    "\n",
    "print(\"ENRICHED SHIPMENT DATA:\")\n",
    "display(df_shipment_enriched)\n",
    "\n",
    "# ============================================================\n",
    "# Verify enriched columns\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA - With Enrichment Columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_enriched.printSchema()\n",
    "\n",
    "print(f\"\\nTotal records: {df_shipment_enriched.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25776626-585b-4dd6-804e-ec30363f3dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COLUMN UNIFORMITY - Standardize Text Case Across Sources\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, lower, upper, initcap\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 1: df_scrubbed (Merged logistics_source1 & source2)\n",
    "# ============================================================\n",
    "\n",
    "# Convert role to LOWERCASE\n",
    "# Convert hub_location to INITCAP (First Letter Capitalized)\n",
    "\n",
    "df_scrubbed_uniform = df_scrubbed.withColumn(\n",
    "    \"role\",\n",
    "    lower(col(\"role\"))\n",
    ").withColumn(\n",
    "    \"hub_location\",\n",
    "    initcap(col(\"hub_location\"))\n",
    ")\n",
    "\n",
    "print(\"STANDARDIZED DATA - df_scrubbed:\")\n",
    "print(\"=\"*60)\n",
    "df_scrubbed_uniform.show()\n",
    "\n",
    "print(\"\\nUnique ROLE values (lowercase):\")\n",
    "df_scrubbed_uniform.select(\"role\").distinct().show()\n",
    "\n",
    "print(\"\\nUnique HUB_LOCATION values (initcap):\")\n",
    "df_scrubbed_uniform.select(\"hub_location\").distinct().show()\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE 2: df_shipment_enriched (JSON shipment details)\n",
    "# ============================================================\n",
    "\n",
    "# Convert vehicle_type to UPPERCASE\n",
    "\n",
    "df_shipment_uniform = df_shipment_enriched.withColumn(\n",
    "    \"vehicle_type\",\n",
    "    upper(col(\"vehicle_type\"))\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STANDARDIZED DATA - df_shipment_enriched:\")\n",
    "print(\"=\"*60)\n",
    "df_shipment_uniform.show()\n",
    "\n",
    "print(\"\\nUnique VEHICLE_TYPE values (uppercase):\")\n",
    "df_shipment_uniform.select(\"vehicle_type\").distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e20526-6e3a-416a-827a-9d88796aaa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format Standardization:<BR>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<BR>\n",
    "Convert shipment_date to yyyy-MM-dd<BR>\n",
    "Ensure shipment_cost has 2 decimal precision<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2a3b5f-b00f-484f-8678-50c307244a80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "df_shipment_formatted."
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FORMAT STANDARDIZATION - Date & Currency Formatting\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, try_to_date, round as spark_round\n",
    "\n",
    "# ============================================================\n",
    "# SOURCE: df_shipment_enriched (logistics_shipment_detail_3000.json)\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Convert shipment_date to yyyy-MM-dd format\n",
    "# Use try_to_date() to safely handle invalid dates\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Ensure shipment_cost has 2 decimal precision\n",
    "# Cast to double and round to 2 decimal places\n",
    "# ============================================================\n",
    "\n",
    "df_shipment_formatted = df_shipment_enriched.withColumn(\n",
    "    \"shipment_date\",\n",
    "    try_to_date(col(\"shipment_date\"), \"dd-MM-yy\")\n",
    ").withColumn(\n",
    "    \"shipment_cost\",\n",
    "    spark_round(col(\"shipment_cost\").cast(\"double\"), 2)\n",
    ")\n",
    "\n",
    "print(\"FORMATTED DATA - Date & Currency Standardized:\")\n",
    "print(\"=\"*60)\n",
    "display(df_shipment_formatted)\n",
    "\n",
    "# ============================================================\n",
    "# VERIFICATION - Check formatting quality\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHIPMENT DATE VALUES (yyyy-MM-dd format):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_formatted.select(\"shipment_date\").show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHIPMENT COST VALUES (2 decimal precision):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_formatted.select(\"shipment_cost\").show(5)\n",
    "\n",
    "# ============================================================\n",
    "# DATE PARSING QUALITY CHECK\n",
    "# ============================================================\n",
    "\n",
    "total_rows = df_shipment_formatted.count()\n",
    "null_dates = df_shipment_formatted.filter(col(\"shipment_date\").isNull()).count()\n",
    "valid_dates = total_rows - null_dates\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {total_rows}\")\n",
    "print(f\"Valid dates: {valid_dates}\")\n",
    "print(f\"Invalid dates (NULL): {null_dates}\")\n",
    "\n",
    "# ============================================================\n",
    "# SCHEMA - Verify data types\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA - Data Types After Formatting\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_shipment_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc9ec1e-dae4-476d-ad74-ad50a8ffdb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE TYPE CASTING - All conversions for merged data\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_type_casted = df_scrubbed_uniform.withColumn(\n",
    "    \"age\",\n",
    "    col(\"age\").cast(\"int\")\n",
    ")\n",
    "\n",
    "print(\"TYPE CASTED DATA:\")\n",
    "display(df_type_casted)\n",
    "\n",
    "print(\"\\nDATATYPES:\")\n",
    "df_type_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e582de-55b0-41d7-a78a-da9f661dbeb1",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769358460943}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "df_renamed"
    }
   },
   "outputs": [],
   "source": [
    "# This works with older Spark versions too\n",
    "df_renamed = df_type_casted.withColumnRenamed(\n",
    "    \"first_name\",\n",
    "    \"staff_first_name\"\n",
    ").withColumnRenamed(\n",
    "    \"last_name\",\n",
    "    \"staff_last_name\"\n",
    ").withColumnRenamed(\n",
    "    \"hub_location\",\n",
    "    \"origin_hub_city\"\n",
    ")\n",
    "\n",
    "display(df_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6d2624-8d2a-4061-b073-de2e3c429683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COLUMN REORDERING - Logical & Standard Format\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ============================================================\n",
    "# COMBINE DATA FROM ALL SOURCES\n",
    "# Join df_renamed (merged sources) with df_shipment_formatted (JSON)\n",
    "# ============================================================\n",
    "\n",
    "# Join on shipment_id\n",
    "df_combined = df_renamed.join(\n",
    "    df_shipment_formatted,\n",
    "    on=\"shipment_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# REORDER COLUMNS - Logical Standard Format\n",
    "# ============================================================\n",
    "\n",
    "# Column Categories:\n",
    "# 1. IDENTIFIER: shipment_id\n",
    "# 2. DIMENSIONS: staff_first_name, staff_last_name, role\n",
    "# 3. LOCATION: origin_hub_city\n",
    "# 4. METRICS: shipment_cost\n",
    "# 5. AUDIT: ingestion_timestamp\n",
    "\n",
    "df_final = df_combined.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"staff_first_name\"),\n",
    "    col(\"staff_last_name\"),\n",
    "    col(\"role\"),\n",
    "    col(\"origin_hub_city\"),\n",
    "    col(\"shipment_cost\"),\n",
    "    col(\"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "print(\"FINAL DATA - Reordered Columns:\")\n",
    "print(\"=\"*60)\n",
    "display(df_final)\n",
    "\n",
    "# ============================================================\n",
    "# VERIFICATION - Schema with logical order\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA - Ordered by Category\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_final.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN ORDER:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, col_name in enumerate(df_final.columns, 1):\n",
    "    print(f\"{i}. {col_name}\")\n",
    "\n",
    "# ============================================================\n",
    "# DATA SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {df_final.count()}\")\n",
    "print(f\"Total columns: {len(df_final.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 5 records:\")\n",
    "df_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26767a9f-521e-4ad1-b9d1-9711930096ac",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769359858369}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769397641402}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DE-DUPLICATION - Remove Duplicates at Record & Key Level\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: RECORD LEVEL DE-DUPLICATION\n",
    "# Remove completely duplicate rows (all columns identical)\n",
    "# ============================================================\n",
    "\n",
    "df_record_dedup = df_final.dropDuplicates()\n",
    "\n",
    "print(\"AFTER RECORD LEVEL DE-DUPLICATION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "original_count = df_final.count()\n",
    "after_record_dedup = df_record_dedup.count()\n",
    "duplicates_removed = original_count - after_record_dedup\n",
    "\n",
    "print(f\"Original records: {original_count}\")\n",
    "print(f\"After removing duplicates: {after_record_dedup}\")\n",
    "print(f\"Duplicate rows removed: {duplicates_removed}\")\n",
    "\n",
    "display(df_record_dedup)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: COLUMN LEVEL DE-DUPLICATION (PRIMARY KEY)\n",
    "# Keep only 1 record per shipment_id (Primary Key)\n",
    "# If multiple records exist for same shipment_id, keep first one\n",
    "# ============================================================\n",
    "\n",
    "# Define window - partition by shipment_id, order by ingestion_timestamp\n",
    "window_spec = Window.partitionBy(\"shipment_id\").orderBy(\"ingestion_timestamp\")\n",
    "\n",
    "# Add row number for each shipment_id\n",
    "df_with_row_num = df_record_dedup.withColumn(\n",
    "    \"row_num\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "# Keep only row_num = 1 (first record per shipment_id)\n",
    "df_pk_dedup = df_with_row_num.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AFTER PRIMARY KEY DE-DUPLICATION (shipment_id):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "after_pk_dedup = df_pk_dedup.count()\n",
    "pk_duplicates_removed = after_record_dedup - after_pk_dedup\n",
    "\n",
    "print(f\"Before PK de-duplication: {after_record_dedup}\")\n",
    "print(f\"After PK de-duplication: {after_pk_dedup}\")\n",
    "print(f\"Duplicate shipment_ids removed: {pk_duplicates_removed}\")\n",
    "\n",
    "display(df_pk_dedup)\n",
    "\n",
    "# ============================================================\n",
    "# VERIFICATION - Check for remaining duplicates\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATA QUALITY CHECK:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for duplicate shipment_ids\n",
    "duplicates_check = df_pk_dedup.groupBy(\"shipment_id\").count().filter(\"count > 1\")\n",
    "\n",
    "print(f\"Duplicate shipment_ids remaining: {duplicates_check.count()}\")\n",
    "\n",
    "if duplicates_check.count() == 0:\n",
    "    print(\"✓ No duplicate shipment_ids (Primary Key Enforced!)\")\n",
    "else:\n",
    "    print(\"⚠️ Duplicate shipment_ids found:\")\n",
    "    duplicates_check.show()\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DE-DUPLICATION SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Starting records: {original_count}\n",
    "After record-level de-dup: {after_record_dedup} (removed {duplicates_removed})\n",
    "After PK de-dup: {after_pk_dedup} (removed {pk_duplicates_removed})\n",
    "\n",
    "Final clean dataset: {after_pk_dedup} records\n",
    "\"\"\")\n",
    "\n",
    "# Final schema\n",
    "print(\"\\nFINAL SCHEMA:\")\n",
    "df_pk_dedup.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce7c598-916d-4d34-8965-1686f3ba3594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c11c6fc-6a20-4207-95b1-500ebefab196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f532be78-9a9a-474f-ade7-cdbbef9e8010",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769396646707}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "df_source_enriched"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA ENRICHMENT - Add Audit Timestamp Column\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, concat, col, lit\n",
    "\n",
    "# Add load_dt column to track when records were ingested\n",
    "df_source_enriched = df_renamed.withColumn(\n",
    "    \"load_dt\",\n",
    "    current_timestamp()\n",
    ").withColumn(\n",
    "    \"full_name\",\n",
    "    concat(col(\"staff_first_name\"), lit(\" \"), col(\"staff_last_name\"))\n",
    ")\n",
    "# Display the data\n",
    "display(df_source_enriched )\n",
    "\n",
    "# Check the new column\n",
    "print(f\"Total records: {df_source_enriched .count()}\")\n",
    "print(f\"Total columns: {len(df_source_enriched .columns)}\")\n",
    "df_source_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0056081d-a69c-417b-8b52-43dfb50fd7bf",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769409485022}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "df_shipment_enriched"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA ENRICHMENT - Time Intelligence, Business Calculations & Splitting\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import concat, col, lit, year, month, day, dayofweek, round as spark_round, when, datediff, current_date, substring\n",
    "\n",
    "# Extract temporal features, status flags, calculate metrics, and split columns\n",
    "df_shipment_enriched = df_shipment_formatted.withColumn(\n",
    "    \"route_segment\",\n",
    "    concat(col(\"source_city\"), lit(\"-\"), col(\"destination_city\"))\n",
    ").withColumn(\n",
    "    \"vehicle_identifier\",\n",
    "    concat(col(\"vehicle_type\"), lit(\"_\"), col(\"shipment_id\"))\n",
    ").withColumn(\n",
    "    \"ship_year\",\n",
    "    year(col(\"shipment_date\"))\n",
    ").withColumn(\n",
    "    \"ship_month\",\n",
    "    month(col(\"shipment_date\"))\n",
    ").withColumn(\n",
    "    \"ship_day\",\n",
    "    day(col(\"shipment_date\"))\n",
    ").withColumn(\n",
    "    \"is_weekend\",\n",
    "    (dayofweek(col(\"shipment_date\")) == 1) | (dayofweek(col(\"shipment_date\")) == 7)\n",
    ").withColumn(\n",
    "    \"shipment_status_flag\",\n",
    "    (col(\"shipment_status\") == \"IN_TRANSIT\") | (col(\"shipment_status\") == \"DELIVERED\")\n",
    ").withColumn(\n",
    "    \"cost_per_kg\",\n",
    "    when(col(\"shipment_weight_kg\") != 0, spark_round(col(\"shipment_cost\") / col(\"shipment_weight_kg\"), 2)).otherwise(None)\n",
    ").withColumn(\n",
    "    \"days_since_shipment\",\n",
    "    datediff(current_date(), col(\"shipment_date\"))\n",
    ").withColumn(\n",
    "    \"tax_amount\",\n",
    "    spark_round(col(\"shipment_cost\") * 0.18, 2)\n",
    ").withColumn(\n",
    "    \"order_prefix\",\n",
    "    substring(col(\"order_id\"), 1, 3)\n",
    ").withColumn(\n",
    "    \"order_sequence\",\n",
    "    substring(col(\"order_id\"), 4, 10)\n",
    ")\n",
    "\n",
    "# Display the data\n",
    "display(df_shipment_enriched)\n",
    "\n",
    "# Check the new columns\n",
    "print(f\"Total records: {df_shipment_enriched.count()}\")\n",
    "print(f\"Total columns: {len(df_shipment_enriched.columns)}\")\n",
    "df_shipment_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd55135-13bd-4061-8d41-1527fc60bc26",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769398700577}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1769399430175}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "df_source_optimized"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA OPTIMIZATION - Remove Redundant Columns\n",
    "# ============================================================\n",
    "\n",
    "# Drop individual name columns (redundant with full_name)\n",
    "df_source_optimized = df_source_enriched.drop(\n",
    "    \"staff_first_name\",\n",
    "    \"staff_last_name\"\n",
    ")\n",
    "\n",
    "# Display the data\n",
    "display(df_source_optimized)\n",
    "\n",
    "# Check removed columns\n",
    "print(f\"Total records: {df_source_optimized.count()}\")\n",
    "print(f\"Total columns: {len(df_source_optimized.columns)}\")\n",
    "df_source_optimized.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61079a91-136e-4ce5-9ed3-d3596d9ba892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c42a83-306d-43e6-8e51-02ec33fd3dfd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769411414969}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CUSTOM BUSINESS LOGIC - Performance Bonus Calculation UDF\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define Python function for bonus calculation\n",
    "def calculate_bonus(role, age):\n",
    "    if role and role.lower() == \"driver\":\n",
    "        if role == 'driver' and age > 30:\n",
    "            return 0.15  # 15% for senior drivers\n",
    "        elif role == 'driver' and age <=30:\n",
    "            return 0.05  # 5% for junior drivers\n",
    "    return 0.0\n",
    "\n",
    "# Register as Spark UDF\n",
    "bonus_udf = udf(calculate_bonus, DoubleType())\n",
    "\n",
    "# Apply UDF to create projected_bonus column\n",
    "df_customized = df_source_optimized.withColumn(\n",
    "    \"projected_bonus\",\n",
    "    bonus_udf(col(\"role\"), col(\"age\"))\n",
    ")\n",
    "\n",
    "# Display the data\n",
    "display(df_customized)\n",
    "\n",
    "# Check the new column\n",
    "print(f\"Total records: {df_customized.count()}\")\n",
    "print(f\"Total columns: {len(df_customized.columns)}\")\n",
    "df_customized.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase_V2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
