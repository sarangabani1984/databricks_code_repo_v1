{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de418ac9-da9d-4a40-8362-c19638bc92e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd26a48-afd9-4032-8923-af2f74dee1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PYSPARK EDA - Logistics Data Analysis\n",
    "# ============================================================\n",
    "\n",
    "# 1. Read CSV and create DataFrame with renamed columns\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,           # First row is column names\n",
    "    inferSchema=True       # Automatically detect data types\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Display the actual data (first 20 rows)\n",
    "# ============================================================\n",
    "display(df)\n",
    "\n",
    "# ============================================================\n",
    "# 3. View the Schema (column names and their data types)\n",
    "# ============================================================\n",
    "print(\"SCHEMA DETAILS:\")\n",
    "df.printSchema()\n",
    "\n",
    "# ============================================================\n",
    "# 4. Show Data Types of each column\n",
    "# ============================================================\n",
    "print(\"\\nDATA TYPES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Show all column names\n",
    "# ============================================================\n",
    "print(\"\\nCOLUMN NAMES:\")\n",
    "print(df.columns)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Check for Duplicate Records (by Ship_id)\n",
    "# ============================================================\n",
    "print(\"\\nDUPLICATE CHECK - Ship_ids appearing more than once:\")\n",
    "df.groupBy(\"Ship_id\").count().filter(\"count > 1\").show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Show Summary Statistics (count, mean, stddev, min, max)\n",
    "# ============================================================\n",
    "print(\"\\nDATAFRAME SUMMARY:\")\n",
    "df.summary().show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BONUS: Additional useful checks\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Check total rows\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "\n",
    "# Check null values in each column\n",
    "print(f\"\\nNull Value Count by Column:\")\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce9dc62-432d-411d-8c75-82a8a96e65e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. LOAD BOTH FILES\n",
    "# ============================================================\n",
    "\n",
    "# Load logistics_source1 (master_v1)\n",
    "master_v1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\")\n",
    "\n",
    "# Load logistics_source2 (master_v2)\n",
    "master_v2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").toDF(\"Ship_id\", \"fname\", \"lname\", \"age\", \"role\",\"location\",\"vhicle_type\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MASTER_V1 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v1.show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MASTER_V2 - First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "master_v2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c6cfdb-4abc-469c-a938-7a54d5bbc267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# FIND COMMON SHIPMENT IDs BETWEEN TWO DATASETS\n",
    "# ============================================================\n",
    "\n",
    "# Extract unique Ship_ids from both datasets\n",
    "v1_ids = master_v1.select(\"Ship_id\").distinct()\n",
    "v2_ids = master_v2.select(\"Ship_id\").distinct()\n",
    "\n",
    "# Find common Ship_ids (intersection - present in both)\n",
    "common_ids = v1_ids.intersect(v2_ids)\n",
    "\n",
    "# Display results\n",
    "print(f\"Common Shipment IDs: {common_ids.count()}\")\n",
    "common_ids.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09359f1-b571-4673-a2c3-18ab72781bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "non_numeric=master_v1.filter(col(\"Ship_id\").rlike(\"^[0-9]\"))\n",
    "non_numeric.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d6d030e-0a57-4e9b-ba40-c264c32fae4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you want the COUNT (interview-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2eafa4-f008-446c-b359-54d6cf02d71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "non_numeric_count = master_v1.filter(\n",
    "    ~col(\"Ship_id\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "non_numeric_count2 = master_v2.filter(\n",
    "    ~col(\"Ship_id\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "\n",
    "print(\"Non-numeric Ship_id count:\", non_numeric_count)\n",
    "print(\"Non-numeric Ship_id count:\", non_numeric_count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5991e22-67aa-4f5d-9fd5-64ac8e11536f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "age is not an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38f6d57-6fc0-4bcd-bb43-861f9ddfce98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_age_count = master_v1.filter(\n",
    "    ~col(\"age\").cast(\"string\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "print(\"Non-integer age count:\", invalid_age_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c409e0a-d11a-4f1a-91df-3648a258cbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_age_count2 = master_v2.filter(\n",
    "    ~col(\"age\").cast(\"string\").rlike(\"^[0-9]+$\")\n",
    ").count()\n",
    "\n",
    "print(\"Non-integer age count:\", invalid_age_count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086ef94a-caae-467b-b45a-27986cb25297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff228194-9fbb-4fa9-a83c-771847c904fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4def3372-ff93-41c6-ab42-80ebeeb15e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA INTEGRATION - Combining Two Sources with Schema Alignment\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Read both files without enforcing schema\n",
    "# ============================================================\n",
    "\n",
    "df_s1 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1.txt\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "df_s2 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2.txt\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"SOURCE 1 - Columns:\")\n",
    "print(df_s1.columns)\n",
    "df_s1.show()\n",
    "\n",
    "print(\"\\nSOURCE 2 - Columns:\")\n",
    "print(df_s2.columns)\n",
    "df_s2.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Align columns to canonical schema\n",
    "# ============================================================\n",
    "# Both sources already have: shipment_id, first_name, last_name, age, role\n",
    "# Need to add: hub_location, vehicle_type (if missing)\n",
    "\n",
    "df_s1_aligned = df_s1.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"role\"), \n",
    ")\n",
    "\n",
    "df_s2_aligned = df_s2.select(\n",
    "    col(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"role\"),\n",
    "    col(\"hub_location\"),\n",
    "    col(\"vehicle_type\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Add data_source column\n",
    "# ============================================================\n",
    "\n",
    "df_s1_with_source = df_s1_aligned.withColumn(\"data_source\", lit(\"system1\"))\n",
    "df_s2_with_source = df_s2_aligned.withColumn(\"data_source\", lit(\"system2\"))\n",
    "\n",
    "print(\"\\nSOURCE 1 - With data_source:\")\n",
    "df_s1_with_source.show()\n",
    "\n",
    "print(\"\\nSOURCE 2 - With data_source:\")\n",
    "df_s2_with_source.show()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Merge both dataframes\n",
    "# ============================================================\n",
    "\n",
    "df_merged = df_s1_with_source.unionByName(\n",
    "    df_s2_with_source,\n",
    "    allowMissingColumns=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGED DATA - Final Schema\")\n",
    "print(\"=\"*60)\n",
    "display(df_merged)\n",
    "\n",
    "print(f\"\\nTotal rows: {df_merged.count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase_V2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
