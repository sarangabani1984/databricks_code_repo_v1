{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0019c7-52b8-4d2c-8b8c-afe88912bfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Objective:\n",
    "# Perform passive data munging (data profiling) on logistics_source1 and logistics_source2.\n",
    "# No data cleansing or modification is performed in this notebook.\n",
    "#\n",
    "# We only IDENTIFY and COUNT data quality issues such as:\n",
    "#  - Non-numeric shipment_id\n",
    "#  - Non-integer age\n",
    "#  - Schema drift (fewer / more columns than expected)\n",
    "#  - Common shipment_ids across both datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d058613-375c-43e8-8536-6ec6390dee69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Volume paths (raw source data)\n",
    "source_path_v1 = \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1\"\n",
    "source_path_v2 = \"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc0e645-7347-4ae3-8d4c-4a0ccc3404bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Read Raw Data in PERMISSIVE Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ce7973-7f18-4cd9-9de4-e1cec9809689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_v1 = spark.read.csv(\n",
    "    source_path_v1,\n",
    "    header=True,\n",
    "    mode=\"PERMISSIVE\",\n",
    "    columnNameOfCorruptRecord=\"corrupt_record\"\n",
    ")\n",
    "\n",
    "master_v2 = spark.read.csv(\n",
    "    source_path_v2,\n",
    "    header=True,\n",
    "    mode=\"PERMISSIVE\",\n",
    "    columnNameOfCorruptRecord=\"corrupt_record\"\n",
    ")\n",
    "display(master_v1)\n",
    "display(master_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd54cbc7-37c3-40ea-98e3-b03915551dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Shipment IDs Appearing in BOTH Datasets <BR>\n",
    "Goal:\n",
    "Identify shipment_id values that exist in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60163b06-8b8a-4ff9-8083-d2473f14aadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Identifying shipment_ids that appear in both master_v1 and master_v2\n",
    "# No type casting is applied (passive analysis only)\n",
    "\n",
    "common_shipment_ids = (\n",
    "    master_v1.select(\"shipment_id\").distinct()\n",
    "    .join(\n",
    "        master_v2.select(\"shipment_id\").distinct(),\n",
    "        on=\"shipment_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(common_shipment_ids)\n",
    "print(\"Common shipment_id count:\", common_shipment_ids.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e57baf-0f15-40ea-bd41-04e27ff61452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Records where shipment_id is NON-NUMERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a044689-10e3-4ff3-8e34-70e7c5ac37d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# shipment_id should contain only digits\n",
    "non_numeric_shipment_v1 = master_v1.filter(\n",
    "    ~col(\"shipment_id\").rlike(\"^[0-9]+$\")\n",
    ")\n",
    "\n",
    "non_numeric_shipment_v2 = master_v2.filter(\n",
    "    ~col(\"shipment_id\").rlike(\"^[0-9]+$\")\n",
    ")\n",
    "\n",
    "print(\"Non-numeric shipment_id count (v1):\", non_numeric_shipment_v1.count())\n",
    "print(\"Non-numeric shipment_id count (v2):\", non_numeric_shipment_v2.count())\n",
    "\n",
    "#display(non_numeric_shipment_v1.select(\"shipment_id\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6130dca-44f4-4146-a76e-96e7eddba142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Records where age is NOT an Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23b2819-8a5f-440f-8453-f29e8b4d5649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# age column is expected to be a numeric integer\n",
    "# Values like 'ten', NULL, or empty string are considered invalid\n",
    "\n",
    "non_integer_age_v1 = master_v1.filter(\n",
    "    ~col(\"age\").rlike(\"^[0-9]+$\")\n",
    ")\n",
    "\n",
    "non_integer_age_v2 = master_v2.filter(\n",
    "    ~col(\"age\").rlike(\"^[0-9]+$\")\n",
    ")\n",
    "\n",
    "print(\"Invalid age count (v1):\", non_integer_age_v1.count())\n",
    "print(\"Invalid age count (v2):\", non_integer_age_v2.count())\n",
    "\n",
    "display(non_integer_age_v1.select(\"shipment_id\", \"age\"))\n",
    "display(non_integer_age_v2.select(\"shipment_id\", \"age\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7fd64d-0e9f-4699-babf-83641d102280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rows with MORE Columns than Expected (Schema Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be77d465-83e9-4339-a620-11e4989f184b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3519b926-349a-4fbb-b2d4-eeb8ca5bb93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”¹ 6. Rows with MORE Columns than Expected (Schema Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5e2ea4-35df-46c7-a122-16bf1b356cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rows with extra columns get captured in _corrupt_record\n",
    "# Expected columns:\n",
    "#  - master_v1 : 5 columns\n",
    "#  - master_v2 : 7 columns\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "extra_columns_v1 = master_v1.filter(col(\"_corrupt_record\").isNotNull())\n",
    "extra_columns_v2 = master_v2.filter(col(\"_corrupt_record\").isNotNull())\n",
    "\n",
    "print(\"Rows with MORE columns (v1):\", extra_columns_v1.count())\n",
    "print(\"Rows with MORE columns (v2):\", extra_columns_v2.count())\n",
    "\n",
    "display(extra_columns_v1.select(\"_corrupt_record\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_passive_data_munging.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
