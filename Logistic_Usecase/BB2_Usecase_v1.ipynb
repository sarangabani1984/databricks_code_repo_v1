{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7a7cb0-b7f9-4697-8ab1-540a808d8394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Custom Schema with Corrupt Record Capture\n",
    "# --------------------------------------------------\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"shipment_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)  # ðŸ‘ˆ REQUIRED\n",
    "])\n",
    "\n",
    "df_s1 = (\n",
    "    spark.read\n",
    "        .schema(custom_schema)\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")  # default, but explicit is good\n",
    "        .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "        .csv(\"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source1\")\n",
    ")\n",
    "\n",
    "display(df_s1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62109ed3-17fd-4e26-bb4c-1b99112b31e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_schema = StructType([\n",
    "    StructField(\"shipment_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"hub_location\", StringType(), True),\n",
    "    StructField(\"vehicle_type\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)  # ðŸ‘ˆ REQUIRED\n",
    "])\n",
    "\n",
    "df_s2 = (\n",
    "    spark.read\n",
    "        .schema(custom_schema)\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")  # default, but explicit is good\n",
    "        .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "        .csv(\"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_source2\")\n",
    ")\n",
    "\n",
    "display(df_s2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897ff300-8c9e-4076-86d4-ef9e8a75d9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Combine Source DataFrames\n",
    "# --------------------------------------------------\n",
    "# - Union df_s1 and df_s2 by column name\n",
    "# - Allow missing columns across sources\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_combine = df_s1.unionByName(\n",
    "    df_s2,\n",
    "    allowMissingColumns=True\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Persist Corrupt Records\n",
    "# --------------------------------------------------\n",
    "# - Filter rows where _corrupt_record is present\n",
    "# - Write them to a quarantine location for analysis\n",
    "# --------------------------------------------------\n",
    "\n",
    "(\n",
    "    df_combine\n",
    "        .filter(\"_corrupt_record IS NOT NULL\")\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(\"/Volumes/lakehouse1/dbread/read_volume/logistics/logistics_corrupt\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
