{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93431be-e6b1-4d3d-88b0-556eceb313d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Key Terminology [](url)\n",
    "\n",
    "#### Validation \n",
    "- Persmissive, \n",
    "- dropmalformated, \n",
    "- Failfast\n",
    "\n",
    "#### When you go for Schema Merging/Melting and Schema Evolution?\n",
    " - unionByName,allowMissingColumns (Multiple file from different location)\n",
    "\n",
    "#### Schema Evolution \n",
    "- mergeSchema=True\n",
    "\n",
    "##### Rejection Strategy \n",
    " - columnNameOfCorruptRecord=\"corruptdata\"\n",
    "\n",
    "#####  Multiple files in multiple paths or sub paths\n",
    "- recursiveFileLookup=True,pathGlobFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d953a9-1ddb-459f-a1dc-a9d6727c2580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e2bc28-8dd2-49e0-978b-98e998b74554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Important passive Munging - EDA of schema/structure functions we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519bbb3b-f688-4730-a545-f7cb6a017dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/lakehouse1/dbread/read_volume/commondata/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a81fc7-2a9f-4e44-b746-3fda89bd8a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.read.text(\"/Volumes/lakehouse1/dbread/read_volume/commondata/custsmodified\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d3a13a-c0dc-4e16-8030-ed94c1cc4d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/commondata/custsmodified\", header=False, inferSchema=True).toDF(\"id\", \"name\", \"lname\", \"age\", \"prof\")\n",
    "display(df1)\n",
    "# df1.printSchema()\n",
    "# display([col for col in df1.columns])\n",
    "# print(df1.schema)\n",
    "# display(df1.dtypes)\n",
    "# display(df1.describe())\n",
    "# display(df1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5c730b-0147-4334-af30-51709a135710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"actual count of the data\",df1.count())\n",
    "print(\"de-duplicated record (all columns) count\",df1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",df1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",df1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(df1.describe())\n",
    "display(df1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f133fe63-594f-48d1-8b29-948819c9d618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/commondata/dummy1/custs\")\n",
    "#display(rawdf1.count())\n",
    "\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/commondata/custsmodified\",\"/Volumes/lakehouse1/dbread/read_volume/commondata/custsmodified_NY\",\"/Volumes/lakehouse1/dbread/read_volume/commondata/dummy1/custs\"],pathGlobFilter=\"custs*\",recursiveFileLookup=True)\n",
    "display(rawdf1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311ae42c-ccd6-48f2-9e94-bcf04cb9bff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/commondata/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "display(rawdf1.count())\n",
    "display(rawdf1)\n",
    "\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/commondata/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf2.count())\n",
    "display(rawdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f7634d-a130-4ddd-bb41-81bbcaaa5655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c97dd4fc-6066-4012-a744-3d8ab9773890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Combining Data + Schema Evolution/Merging (Structuring) - Preliminary Datamunging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b0cf912-49b8-4b5b-8b3f-6ca65fea206f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####**Single File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6be8991-06ce-41ee-b460-ca9e5e06df4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\")\n",
    "print(f\"Single file total count\",rawdf1.count())\n",
    "#display(rawdf1)\n",
    "\n",
    "#Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",\"/Volumes/lakehouse1/dbread/read_volume/custsmodified_NY\",\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified_TX\"])\n",
    "print(f\"Multiple files (with different names)\",rawdf1.count())\n",
    "#display(rawdf1)\n",
    "\n",
    "#Multiple files (with different names, recursive)\n",
    "rawdf1 = spark.read.schema(struct1).csv(\n",
    "    path=\"/Volumes/lakehouse1/dbread/read_volume/\",\n",
    "    pathGlobFilter=\"custs*\",\n",
    "    recursiveFileLookup=True\n",
    ")\n",
    "print(f\"Multiple files (with different names, recursive)\",rawdf1.count())\n",
    "display(rawdf1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bf0828-0c22-4cb4-944b-f6afadf80f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schem1 = \"id string, firstname string, lastname string, age int, profession string\"\n",
    "rawdf1=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified_NY\",schema=schem1)\n",
    "display(rawdf1)\n",
    "\n",
    "schem2 = \"id string, firstname string, age int, profession string, city string\"\n",
    "rawdf2 =spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified_TX\",schema=schem2)\n",
    "display(rawdf2)\n",
    "\n",
    "rawdf_merged= rawdf1.unionByName(rawdf2, allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "#rawdf_merged=rawdf1.unionByName(rawdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a64dfa7-a462-4c97-932b-985364aa07b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Validation â€“ Data Exploration through Cleansing and Scrubbing\n",
    "\n",
    "- **Scrubbing**: Applied **Permissive mode** to handle unexpected data types by converting invalid values to **NULL**.\n",
    "- **Cleansing**: Applied **Drop Malformed mode** to eliminate records containing invalid or malformed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b25302-1ae6-4667-a81f-3c5e2e36d1d3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766987433969}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766987433982}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strt1=\"id int, firstname string, lastname string, age int, profession string\"\n",
    "\n",
    "df_raw=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\")\n",
    "df_raw.show(20)\n",
    "\n",
    "strt11=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True),StructField(\"corruptdata\",StringType(),True)])\n",
    "\n",
    "dfmethod1=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\", schema=strt11,mode=\"PERMISSIVE\",header=False)\n",
    "\n",
    "print(\"dfmethod1 entire count of data\",dfmethod1.count())\n",
    "print(\"dfmethod1 after scrubbing, count of data\",len(dfmethod1.collect()))\n",
    "display(dfmethod1)\n",
    "\n",
    "\n",
    "dfmethod2=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\", schema=strt11,mode=\"dropMalformed\",header=False)\n",
    "\n",
    "print(\"dfmethod2 entire count of data\",dfmethod2.count())\n",
    "print(\"dfmethod2 after scrubbing, count of data\",len(dfmethod2.collect()))\n",
    "display(dfmethod2)\n",
    "\n",
    "dfmethod3 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\",\n",
    "    schema=strt11,\n",
    "    mode=\"PERMISSIVE\",\n",
    "    header=False,\n",
    "    columnNameOfCorruptRecord=\"corruptdata\"\n",
    ")\n",
    "\n",
    "print(\"dfmethod3 entire count of data\",dfmethod3.count())\n",
    "print(\"dfmethod3 after scrubbing, count of data\",len(dfmethod3.collect()))\n",
    "display(dfmethod3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2196291-90db-4d5b-8638-7437c5e0d2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before actively Cleansing or Scrubbing - We have to create a Rejection Strategy to reduce data challenges in the future\n",
    "strt11 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"profession\", StringType(), True),\n",
    "    StructField(\"corruptdata\", StringType(), True)\n",
    "])\n",
    "\n",
    "dfmethod3 = spark.read.schema(strt11).csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    header=False,\n",
    "    columnNameOfCorruptRecord=\"corruptdata\"\n",
    ")\n",
    "\n",
    "display(dfmethod3)\n",
    "\n",
    "\n",
    "print(\"entire count of data\", dfmethod3.count())\n",
    "df_reject = dfmethod3.where(\"corruptdata is not null\")\n",
    "df_reject.drop(\"corruptdata\").write.mode(\"overwrite\").option(\"header\", True).option(\"delimiter\", \",\").csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/rejects/\")\n",
    "print(\"Data to reject or update the source\", df_reject.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed397b35-0cbc-4b8e-b02c-40a80def9282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing\n",
    "It is a process of cleaning/removing or making the data more clean Eg. Cutting/removing debris portion of the potato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1449b06-b868-4f1f-b7bb-ecf954af4555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleansed_df1=dfmethod3.na.drop(how=\"any\")#drop the row, if any one column in our df row contains null\n",
    "#cleansed_df1=dfmethod3.na.drop(how=\"any\",subset=[\"id\",\"age\"])#drop the row, if any one column id/age contains null\n",
    "print(\"cleansed any DF count\",len(cleansed_df1.collect()))\n",
    "display(cleansed_df1.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b031ea5-b6c4-4f83-a577-8bd177522c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cleansed_df2=dfmethod3.na.drop(how=\"all\")#drop the row, if all the columns in our df row contains null\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\",subset=[\"id\",\"profession\"])#drop the row, if all the columns (id,profession) in our df row contains null\n",
    "print(\"cleansed all DF count\",len(cleansed_df2.collect()))\n",
    "display(cleansed_df2.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82788c62-e09c-404c-a39f-5503c64ebb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before scrubbing, lets take the right cleansed data with id as null and entire row as null removed out\n",
    "#Finally I am arriving for our current data, lets perform the best cleansing\n",
    "cleansed_df=dfmethod3.na.drop(subset=[\"id\"]).na.drop(how=\"all\")\n",
    "print(\"Final cleansed DF\",len(cleansed_df.collect()))\n",
    "display(cleansed_df.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a1cf1f-7e3a-4653-9f5f-762944f6e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbed_df1=cleansed_df.na.fill(\"na\",subset=[\"firstname\",\"lastname\"]).na.fill(\"not provided\",subset=[\"profession\"])\n",
    "scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "display(scrubbed_df2.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ae4f48-92a8-4cb4-bdbe-1afa24972567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict1={\"IT\":\"Information Technologies\",\"Pilot\":\"Aircraft Pilot\",\"Actor\":\"Celebrity\"}\n",
    "scrubbed_df=scrubbed_df1.na.replace(dict1,subset=[\"profession\"])\n",
    "print(\"scrubbed DF\",len(scrubbed_df.collect()))\n",
    "display(scrubbed_df.take(15))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
