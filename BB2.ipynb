{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93431be-e6b1-4d3d-88b0-556eceb313d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Key Terminology [](url)\n",
    "\n",
    "#### Validation \n",
    "- Persmissive, \n",
    "- dropmalformated, \n",
    "- Failfast\n",
    "\n",
    "#### When you go for Schema Merging/Melting and Schema Evolution?\n",
    " - unionByName,allowMissingColumns (Multiple file from different location)\n",
    "\n",
    "#### Schema Evolution \n",
    "- mergeSchema=True\n",
    "\n",
    "##### Rejection Strategy \n",
    " - columnNameOfCorruptRecord=\"corruptdata\"\n",
    "\n",
    "#####  Multiple files in multiple paths or sub paths\n",
    "- recursiveFileLookup=True,pathGlobFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d953a9-1ddb-459f-a1dc-a9d6727c2580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e2bc28-8dd2-49e0-978b-98e998b74554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Important passive Munging - EDA of schema/structure functions we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519bbb3b-f688-4730-a545-f7cb6a017dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/lakehouse1/dbread/read_volume/sub/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a81fc7-2a9f-4e44-b746-3fda89bd8a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.read.text(\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d3a13a-c0dc-4e16-8030-ed94c1cc4d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\", header=False, inferSchema=True).toDF(\"id\", \"name\", \"lname\", \"age\", \"prof\")\n",
    "display(df1)\n",
    "# df1.printSchema()\n",
    "# display([col for col in df1.columns])\n",
    "# print(df1.schema)\n",
    "# display(df1.dtypes)\n",
    "# display(df1.describe())\n",
    "# display(df1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5c730b-0147-4334-af30-51709a135710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"actual count of the data\",df1.count())\n",
    "print(\"de-duplicated record (all columns) count\",df1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",df1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",df1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(df1.describe())\n",
    "display(df1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f133fe63-594f-48d1-8b29-948819c9d618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custs\")\n",
    "#display(rawdf1.count())\n",
    "\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",\"/Volumes/lakehouse1/dbread/read_volume/custsmodified_NY\",\"/Volumes/lakehouse1/dbread/read_volume/custs\"],pathGlobFilter=\"custs*\",recursiveFileLookup=True)\n",
    "display(rawdf1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311ae42c-ccd6-48f2-9e94-bcf04cb9bff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "display(rawdf1.count())\n",
    "display(rawdf1)\n",
    "\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf2.count())\n",
    "display(rawdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f7634d-a130-4ddd-bb41-81bbcaaa5655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97dd4fc-6066-4012-a744-3d8ab9773890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Combining Data + Schema Evolution/Merging (Structuring) - Preliminary Datamunging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0cf912-49b8-4b5b-8b3f-6ca65fea206f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####**Single File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6be8991-06ce-41ee-b460-ca9e5e06df4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\")\n",
    "print(f\"Single file total count\",rawdf1.count())\n",
    "#display(rawdf1)\n",
    "\n",
    "#Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",\"/Volumes/lakehouse1/dbread/read_volume/custsmodified_NY\",\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified_TX\"])\n",
    "print(f\"Multiple files (with different names)\",rawdf1.count())\n",
    "#display(rawdf1)\n",
    "\n",
    "#Multiple files (with different names, recursive)\n",
    "rawdf1 = spark.read.schema(struct1).csv(\n",
    "    path=\"/Volumes/lakehouse1/dbread/read_volume/\",\n",
    "    pathGlobFilter=\"custs*\",\n",
    "    recursiveFileLookup=True\n",
    ")\n",
    "print(f\"Multiple files (with different names, recursive)\",rawdf1.count())\n",
    "display(rawdf1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bf0828-0c22-4cb4-944b-f6afadf80f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schem1 = \"id string, firstname string, lastname string, age int, profession string\"\n",
    "rawdf1=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified_NY\",schema=schem1)\n",
    "display(rawdf1)\n",
    "\n",
    "schem2 = \"id string, firstname string, age int, profession string, city string\"\n",
    "rawdf2 =spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified_TX\",schema=schem2)\n",
    "display(rawdf2)\n",
    "\n",
    "rawdf_merged= rawdf1.unionByName(rawdf2, allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "#rawdf_merged=rawdf1.unionByName(rawdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a64dfa7-a462-4c97-932b-985364aa07b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Validation – Data Exploration through Cleansing and Scrubbing\n",
    "\n",
    "- **Scrubbing**: Applied **Permissive mode** to handle unexpected data types by converting invalid values to **NULL**.\n",
    "- **Cleansing**: Applied **Drop Malformed mode** to eliminate records containing invalid or malformed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b25302-1ae6-4667-a81f-3c5e2e36d1d3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766987433969}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strt1=\"id int, firstname string, lastname string, age int, profession string\"\n",
    "\n",
    "df_raw=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\")\n",
    "df_raw.show(20)\n",
    "\n",
    "strt11=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True),StructField(\"corruptdata\",StringType(),True)])\n",
    "\n",
    "dfmethod1=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\", schema=strt11,mode=\"PERMISSIVE\",header=False)\n",
    "\n",
    "print(\"dfmethod1 entire count of data\",dfmethod1.count())\n",
    "print(\"dfmethod1 after scrubbing, count of data\",len(dfmethod1.collect()))\n",
    "display(dfmethod1)\n",
    "\n",
    "\n",
    "dfmethod2=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\", schema=strt11,mode=\"dropMalformed\",header=False)\n",
    "\n",
    "print(\"dfmethod2 entire count of data\",dfmethod2.count())\n",
    "print(\"dfmethod2 after scrubbing, count of data\",len(dfmethod2.collect()))\n",
    "display(dfmethod2)\n",
    "\n",
    "dfmethod3 = spark.read.csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\",\n",
    "    schema=strt11,\n",
    "    mode=\"PERMISSIVE\",\n",
    "    header=False,\n",
    "    columnNameOfCorruptRecord=\"corruptdata\"\n",
    ")\n",
    "\n",
    "print(\"dfmethod3 entire count of data\",dfmethod3.count())\n",
    "print(\"dfmethod3 after scrubbing, count of data\",len(dfmethod3.collect()))\n",
    "display(dfmethod3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2196291-90db-4d5b-8638-7437c5e0d2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before actively Cleansing or Scrubbing - We have to create a Rejection Strategy to reduce data challenges in the future\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType\n",
    ")\n",
    "\n",
    "strt11 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"profession\", StringType(), True),\n",
    "    StructField(\"corruptdata\", StringType(), True)\n",
    "])\n",
    "\n",
    "dfmethod3 = spark.read.schema(strt11).csv(\n",
    "    \"/Volumes/lakehouse1/dbread/read_volume/sub/custsmodified\",\n",
    "    mode=\"PERMISSIVE\",\n",
    "    header=False,\n",
    "    columnNameOfCorruptRecord=\"corruptdata\"\n",
    ")\n",
    "\n",
    "display(dfmethod3)\n",
    "\n",
    "\n",
    "print(\"entire count of data\", dfmethod3.count())\n",
    "df_reject = dfmethod3.where(\"corruptdata is not null\")\n",
    "df_reject.drop(\"corruptdata\").write.mode(\"overwrite\").option(\"header\", True).option(\"delimiter\", \",\").csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/rejects/\")\n",
    "print(\"Data to reject or update the source\", df_reject.count())\n",
    "display(df_reject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c44ad1-ea88-45f3-a2a9-68f4e99a7b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read_back = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"/Volumes/lakehouse1/dbread/read_volume/sub/rejects/\")\n",
    "\n",
    "display(df_read_back)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed397b35-0cbc-4b8e-b02c-40a80def9282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing\n",
    "It is a process of cleaning/removing or making the data more clean Eg. Cutting/removing debris portion of the potato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1449b06-b868-4f1f-b7bb-ecf954af4555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleansed_df1=dfmethod3.na.drop(how=\"any\")#drop the row, if any one column in our df row contains null\n",
    "#cleansed_df1=dfmethod3.na.drop(how=\"any\",subset=[\"id\",\"age\"])#drop the row, if any one column id/age contains null\n",
    "print(\"cleansed any DF count\",len(cleansed_df1.collect()))\n",
    "display(cleansed_df1.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b031ea5-b6c4-4f83-a577-8bd177522c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cleansed_df2=dfmethod3.na.drop(how=\"all\")#drop the row, if all the columns in our df row contains null\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\",subset=[\"id\",\"profession\"])#drop the row, if all the columns (id,profession) in our df row contains null\n",
    "print(\"cleansed all DF count\",len(cleansed_df2.collect()))\n",
    "display(cleansed_df2.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82788c62-e09c-404c-a39f-5503c64ebb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before scrubbing, lets take the right cleansed data with id as null and entire row as null removed out\n",
    "#Finally I am arriving for our current data, lets perform the best cleansing\n",
    "cleansed_df = (\n",
    "    dfmethod3\n",
    "        .filter(\"corruptdata is null\")\n",
    "        .na.drop(subset=[\"id\"])\n",
    "        .na.drop(how=\"all\")\n",
    "        .drop(\"corruptdata\")\n",
    ")\n",
    "\n",
    "print(\"Final cleansed DF\",len(cleansed_df.collect()))\n",
    "display(cleansed_df.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a1cf1f-7e3a-4653-9f5f-762944f6e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767156286400}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbed_df1=cleansed_df.na.fill(\"na\",subset=[\"firstname\",\"lastname\"]).na.fill(\"not provided\",subset=[\"profession\"])\n",
    "scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "display(scrubbed_df2.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ae4f48-92a8-4cb4-bdbe-1afa24972567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict1={\"IT\":\"Information Technologies\",\"Pilot\":\"Doctor\",\"Actor\":\"Business\"}\n",
    "scrubbed_df=scrubbed_df1.na.replace(dict1,subset=[\"profession\"])\n",
    "print(\"scrubbed DF\",len(scrubbed_df.collect()))\n",
    "display(scrubbed_df.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9e47fb-4a39-4765-8fb4-944eda240c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42ab4fc-0764-4f9b-8258-7af3c631e49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap\n",
    "standard_df1=scrubbed_df.withColumn(\"Source_sys\", lit(\"Retail\"))\n",
    "display(standard_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaef2546-1d4b-4917-8023-b425ceb7cce3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767158019389}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardization2 - UNIFORMITY of the data\n",
    "display(standard_df1.groupBy(\"profession\").count())#DSL\n",
    "#standard_df1.createOrReplaceTempView(\"view1\")\n",
    "#display(spark.sql(\"select profession,count(1) from view1 group by profession\"))#Declarative lang\n",
    "standard_df2=standard_df1.withColumn(\"profession\",initcap(\"profession\"))#If we have to add a columns with some hardcoded value in dataframe, we have use lit function to add a hardcoded/literal value\n",
    "#display(standard_df2.take(15))\n",
    "display(standard_df2.groupBy(\"profession\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245d4d20-0df8-4cc4-97dc-16f7724578f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Standardization 3 – Format Standardization\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Mapping for ID standardization\n",
    "# (Can later be enhanced using GenAI)\n",
    "cid_standardization = {\n",
    "    \"one\": \"1\",\n",
    "    \"two\": \"2\",\n",
    "    \"ten\": \"10\"\n",
    "}\n",
    "\n",
    "# Replace textual IDs with numeric equivalents\n",
    "# Using NA replace (data munging technique)\n",
    "standard_df3 = (\n",
    "    standard_df2\n",
    "        .na.replace(cid_standardization, subset=[\"id\"])\n",
    ")\n",
    "\n",
    "# Standardize age format by removing '-' character\n",
    "standard_df3 = (\n",
    "    standard_df3\n",
    "        .withColumn(\"age\", regexp_replace(\"age\", \"-\", \"\"))\n",
    ")\n",
    "\n",
    "# Preview standardized data\n",
    "display(standard_df3.take(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80952d1f-a264-406a-9a7c-828942211986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "standard_df3.printSchema()\n",
    "#display(len(standard_df3.collect()))\n",
    "#display(standard_df3.where(\"id like '%trailer%'\"))\n",
    "standard_df3=standard_df3.where(\"id not rlike '[a-zA-Z]'\")#Removed the string data in the id column\n",
    "#display(standard_df3.where(\"id='trailer_data:end of file'\"))\n",
    "#display(len(standard_df3.collect()))\n",
    "standard_df4=standard_df3.withColumn(\"age\",col(\"age\").cast(\"int\")).withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standard_df4.printSchema()\n",
    "display(standard_df4.take(15))\n",
    "#standard_df4.where(\"id=1000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220c9c1c-af6b-46f8-b8b4-f20d7ad0a357",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767160113936}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df5=standard_df4.withColumnsRenamed({\"id\":\"custid\",\"Source_sys\":\"sourcesystem\"})\n",
    "standard_df5.printSchema()\n",
    "display(standard_df5.take(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ca8eaf-653b-4475-9452-1766b612ddb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df6=standard_df5.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcesystem\")\n",
    "standard_df6.printSchema()\n",
    "display(standard_df6.take(15))\n",
    "standard_df6.write.mode(\"overwrite\").saveAsTable(\"lakehouse1.dbread.munged_cust_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ab5d17-d403-4368-a188-90cffb9ff500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dedup_df1 = standard_df6.where(\"custid IN (4000001, 4000003)\")\n",
    "#display(dedup_df1)\n",
    "dedup_df1 = standard_df6.distinct().orderBy(\"custid\")\n",
    "display(dedup_df1)\n",
    "#standard_df6.write.mode(\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df96f691-7c9b-41a5-9734-9688741241e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dedup_df2 = (\n",
    "    dedup_df1\n",
    "        .orderBy([\"custid\", \"age\"], ascending=[True, False])\n",
    "        .coalesce(1)\n",
    "        \n",
    ")\n",
    "\n",
    "display(dedup_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced94576-91ad-4d47-9bb8-7d06df967894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dedup_df3=dedup_df2.dropDuplicates([\"custid\"])\n",
    "display(dedup_df3)\n",
    "dedup_df4 = (\n",
    "    dedup_df1\n",
    "        .dropDuplicates([\"custid\", \"age\", \"firstname\", \"lastname\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9381f2c9-889c-497e-ace9-99398b826314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before we enrich, lets do some EDA\n",
    "#Every stages we need to do basic EDA (Data Exploration)\n",
    "dedup_df3.printSchema()\n",
    "print(\"Records got cleaned/munged \",dfmethod1.count()-len(dedup_df3.collect()))\n",
    "display(dedup_df3.summary())\n",
    "display(dedup_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799c2908-e45f-4da1-a11a-f2550a9673d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Enrichment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23956481-76d7-41e9-a92d-c09f013df647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### WithColumn Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5b4fd9-b199-4476-97dc-79449c1cead9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "datadt = '2026-01-01'\n",
    "enrich_df1 = dedup_df3.withColumn(\"loaddt\", current_date()).withColumn(\"datadt\", lit(datadt))\n",
    "display(enrich_df1.limit(2))\n",
    "\n",
    "enrich_df1 = dedup_df3.withColumns({\"loaddt\": current_date(), \"datadt\": lit(datadt)})\n",
    "display(enrich_df1.limit(2))\n",
    "\n",
    "enrich_df1 = dedup_df3.select('*',current_date().alias('loaddt'),lit(datadt).alias('datadt'))\n",
    "display(enrich_df1.limit(2))\n",
    "\n",
    "enrich_df1 = dedup_df3.selectExpr('*',\"current_date() as loaddt\",\"'2026-01-01' as datadt\")\n",
    "display(enrich_df1.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82f5935-5f96-41c0-ae7e-0b4d6c6327df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,upper,lower\n",
    "\n",
    "enrich_df2 = enrich_df1.withColumn(\n",
    "    \"firstnames\",\n",
    "    upper(col(\"firstname\"))).withColumn(\n",
    "    \"firstname\",\n",
    "    lower(col(\"firstname\"))).withColumn(\n",
    "        \"last_name\",col(\"lastname\"))\n",
    "\n",
    "enrich_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "101e71b1-3af1-4b38-95a5-e32b95834c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e161a568-c2d8-4bfb-8675-05ec83367e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "enrich_df2=enrich_df1.withColumn(\"profession_flag\",substring(\"profession\",1,1))\n",
    "enrich_df2.show(10)\n",
    "#or we can achieve using select\n",
    "enrich_df2=enrich_df1.select(\"*\",substring(\"profession\",1,1).alias(\"profession_flag\"))\n",
    "enrich_df2.show(10)\n",
    "#or we can achieve using selectExpr\n",
    "enrich_df2=enrich_df1.selectExpr(\"*\",\"substr(profession,1,1) as profession_flag\")\n",
    "enrich_df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4219d00-ea34-4931-9b03-a355e477740c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1422214d-798d-4c18-94c6-56d938713ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df3=enrich_df2.withColumnRenamed(\"profession_flag\",\"proflag\")#better to use\n",
    "enrich_df3.show(10)\n",
    "enrich_df3=enrich_df2.withColumnsRenamed({\"profession_flag\":\"proflag\",\"profession\":\"prof\"})\n",
    "enrich_df3.show(10)\n",
    "#or\n",
    "enrich_df3=enrich_df2.select(\"*\",col(\"profession\").alias(\"prof\"))#This will derive a new column called prof\n",
    "enrich_df3=enrich_df2.select(\"custid\",\"age\",\"firstname\",\"lastname\",col(\"profession\").alias(\"prof\"),\"sourcesystem\",\"loaddt\",\"datadt\",col(\"profession_flag\").alias(\"proflag\"))#This will derive a new column called prof#better to use\n",
    "#enrich_df3=enrich_df3.drop(\"profession\")#column orders are changed, not good to use\n",
    "enrich_df3.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b8e3b2-844c-4f09-9368-f32351e2af9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804febd0-4d15-480a-97cd-9bc87f60d544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, upper, col\n",
    "\n",
    "enrich_df3.printSchema()  # datadt is not in expected date format for further usage and I want to convert sourcesystem into uppercase\n",
    "enrich_df4 = (\n",
    "    enrich_df3\n",
    "    .withColumn(\"sourcesystem\", upper(col(\"sourcesystem\")))\n",
    "    .withColumn(\"datadt\", to_date(col(\"datadt\"), 'yyyy-MM-dd'))\n",
    ")\n",
    "enrich_df4.printSchema()  # datadt is expected date format\n",
    "display(enrich_df4.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bff2a500-f72c-4ca7-b09a-6a30f6f4a7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8748c0fc-9a68-4ddd-8d17-4820b27eed35",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767272629423}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,upper\n",
    "enrich_df5=enrich_df4.withColumn(\"fullname\",upper(concat(col(\"firstname\"),lit(\"_\") , col(\"lastname\"))))\n",
    "enrich_df5=enrich_df5.drop(\"firstname\",\"lastname\").select(\"custid\",\"age\",\"fullname\",\"prof\",\"sourcesystem\",\"loaddt\",\"datadt\",\"proflag\")\n",
    "display(enrich_df5.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ddf1179-7e03-408b-924d-6ff3d05b6c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "===============================\n",
    "PySpark Column Enrichment\n",
    "Conclusion / Best Practices\n",
    "===============================\n",
    "\n",
    "1) select()  ⭐ BEST FOR FINAL SHAPE\n",
    "----------------------------------\n",
    "Use when you want:\n",
    "- Reorder / order columns\n",
    "- Drop unwanted columns\n",
    "- Derive / reformat columns\n",
    "- Rename columns using alias()\n",
    "- Perform ALL operations in ONE iteration\n",
    "\n",
    "Notes:\n",
    "- alias() derives a new column (not a true rename)\n",
    "- Must include \"*\" if you want to keep all columns\n",
    "- Best for Gold layer & final output\n",
    "\n",
    "Example:\n",
    "df.select(\"*\", col(\"a\").alias(\"b\"))\n",
    "\n",
    "\n",
    "2) selectExpr()  ⭐ SQL STYLE\n",
    "----------------------------\n",
    "Use when you want:\n",
    "- Same capabilities as select()\n",
    "- SQL-style expressions instead of DSL\n",
    "- Multiple operations in ONE iteration\n",
    "\n",
    "Notes:\n",
    "- String-based (less type-safe)\n",
    "- Good for SQL-heavy teams\n",
    "\n",
    "Example:\n",
    "df.selectExpr(\"*\", \"a as b\", \"upper(name) as uname\")\n",
    "\n",
    "\n",
    "3) withColumn()  ⭐ INCREMENTAL ENRICHMENT\n",
    "----------------------------------------\n",
    "Use when you want:\n",
    "- Add new columns\n",
    "- Derive columns\n",
    "- Modify / replace existing columns\n",
    "\n",
    "Notes:\n",
    "- Same column name → replaces column\n",
    "- New column added at the end\n",
    "- Renaming via withColumn is NOT recommended\n",
    "- Dropping columns is NOT possible\n",
    "\n",
    "Example:\n",
    "df.withColumn(\"age2\", col(\"age\") + 1)\n",
    "\n",
    "\n",
    "4) withColumnRenamed()  ⭐ RENAME ONLY\n",
    "------------------------------------\n",
    "Use when you want:\n",
    "- Rename a column cleanly and safely\n",
    "\n",
    "Notes:\n",
    "- Does not change column order\n",
    "- No recomputation\n",
    "\n",
    "Example:\n",
    "df.withColumnRenamed(\"old\", \"new\")\n",
    "\n",
    "\n",
    "5) drop()  ⭐ REMOVE ONLY\n",
    "-----------------------\n",
    "Use when you want:\n",
    "- Remove one or more columns quickly\n",
    "\n",
    "Notes:\n",
    "- No expressions allowed\n",
    "- Avoid after select if order/schema matters\n",
    "\n",
    "Example:\n",
    "df.drop(\"col1\", \"col2\")\n",
    "\n",
    "\n",
    "===============================\n",
    "FINAL STICKY SUMMARY\n",
    "===============================\n",
    "\n",
    "Enrich step-by-step  → withColumn\n",
    "Rename cleanly       → withColumnRenamed\n",
    "Remove columns       → drop\n",
    "Finalize schema      → select\n",
    "SQL users            → selectExpr\n",
    "\n",
    "\n",
    "ONE-LINE MEMORY:\n",
    "\"Enrich early with withColumn, finalize late with select\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9746221-3baf-4837-9ef3-e9bf88d2f11f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767274197143}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#Splitting of columns\n",
    "enrich_df6=enrich_df5.withColumn(\"profsplit\",split(col(\"prof\"),' '))\n",
    "enrich_df6=enrich_df6.withColumns({\"proffirst\":col(\"profsplit\")[0],\"proflast\":col(\"profsplit\")[size(col(\"profsplit\"))-1]})\n",
    "display(enrich_df6.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d79100-d4eb-424f-a345-ba98ed29f4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Merging of columns\n",
    "enrich_df7=enrich_df6.withColumn(\"proflag\",concat(substring(col(\"proffirst\"),1,1),substring(col(\"proflast\"),1,1))).drop(\"profsplit\")\n",
    "display(enrich_df7)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
