{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1b18ad-3c94-4839-9146-7216dc1d8ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/lakehouse1/dbread/read_volume/custs\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1.createOrReplaceTempView(\"v1\")\n",
    "spark.sql(\"select * from v1 where age >50\").createOrReplaceTempView(\"_sqldf\")\n",
    "\n",
    "spark.sql(\"create or replace temp view v2 as select * from v1 where age <50\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0324fa7-6d15-4aee-8ff0-d9bbf80b31cf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769436587774}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ce7966-4c80-400d-bb6e-1bf0a54fa26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1.printSchema()\n",
    "print(rawdf1.columns)\n",
    "rawdf1.dtypes\n",
    "print(rawdf1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea87ed58-943b-49c8-9434-a4f411704f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a7e0bc-4660-4d84-91fe-6a72149c50f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1.createOrReplaceTempView(\"rawdf1view\")\n",
    "print(\"actual count of the data\",\"select count(*) from rawdf1view\")\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d60335-b03c-41d8-a497-9141d1fb5cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS dedup_all_columns_count \n",
    "FROM (SELECT distinct * FROM rawdf1view) t;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d78c3e0-e3e6-47a9-a322-7542ae1ba918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS dedup_all_columns_count \n",
    "FROM (SELECT id FROM rawdf1view where id is not null) t;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9a4e42-7b96-44fb-93fe-323fc88412c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession#15lakhs\n",
    "spark=SparkSession.builder.appName(\"WD36 - ETL Pipeline - Bread & Butter\").getOrCreate()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8736f252-10bd-48b5-ad17-6f0d47d577d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\")\n",
    "#2. Multiple files (with same or different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\",\"/Volumes/workspace/wd36schema/ingestion_volume/staging/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef32bd34-5dd1-4772-8a02-469120aced4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "#print(rawdf1.schema)\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleandf1.count())#all rows count\n",
    "display(cleandf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",mode='dropMalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleandf1.collect()))\n",
    "display(cleandf1)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)\n",
    "print(cleandf1.count())#count will return the original count of the raw data\n",
    "print(len(cleandf1.collect()))#collect+len will return the dropmalformed count of the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e5f174-34ff-42c1-a456-bd3edba99921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struttype1=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf1=spark.read.schema(struttype1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf1.count())#all rows count\n",
    "display(rawdf1)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b65831b-1123-4b3d-bb6b-764ec1e5db63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating rejection dataset to send to our source system for future fix\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/lakehouse1/dbread/read_volume/custsmodified\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "#Create a reject dataset\n",
    "rejectdf1=cleandf1.where(\"corruptedrows is not null\")\n",
    "#display(rejectdf1)\n",
    "rejectdf1.write.csv(\"/Volumes/lakehouse1/dbread/read_volume/Rejected/\",mode=\"overwrite\",header=True)\n",
    "retaineddf1=cleandf1.where(\"corruptedrows is null\")\n",
    "print(\"Overall rows in the source data is \",len(cleandf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(rejectdf1.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retaineddf1.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53b4241-45de-469c-8c06-378d54b13c04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "df_removed_corrupted"
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf=cleandf1.filter(\"corruptedrows is null\").drop(\"corruptedrows\")\n",
    "cleanseddf.createOrReplaceTempView(\"temp\")\n",
    "# spark.sql(\"select *, count(*) cnt from temp group by 1,2,3,4,5 having cnt>1\").show()\n",
    "#display(cleanseddf)\n",
    "print(\"na.drop\",cleanseddf.na.drop(how='any').count())\n",
    "print(\"distinct\",cleanseddf.distinct().count())\n",
    "#cleanseddf.na.drop(how='any',subset=['id','lastname']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f595d8-8b61-4622-8d42-2e3308f174d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dsl_na.fill"
    }
   },
   "outputs": [],
   "source": [
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4855b0e7-0eeb-4ce8-bf48-8060120fd804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "cleanseddf \\\n",
    "    .na.fill(\"Not Provided\", subset=[\"firstname\", \"lastname\", \"profession\"]) \\\n",
    "    .na.fill(-1, subset=[\"age\"]) \\\n",
    "    .na.replace(find_replace_values_dict1,subset=[\"profession\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3231551c-d227-45e8-ae3b-5f7d416bb519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf.createOrReplaceTempView(\"temp1\")\n",
    "\n",
    "spark.sql(\"create or replace temp view temp2 as select id, firstname, coalesce(lastname,'n/a') lastname, age, coalesce(profession,'n/a') profession from temp1\").show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f224e7e5-6564-42b5-ae00-f75074d26641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf=rawdf1.na.drop(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3802ad61-e144-4dab-bfbf-9f9b2b4e4e64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "start from here"
    }
   },
   "outputs": [],
   "source": [
    "scrubbeddf1=cleanseddf.na.fill('not provided',subset=[\"lastname\",\"profession\"])#fill will help us replace nulls with some value\n",
    "display(scrubbeddf1)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "scrubbeddf2=scrubbeddf1.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is helping us find and replace the values\n",
    "scrubbeddf3=scrubbeddf2.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3565de-85b7-425a-9196-f9495ac08ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf3.where(\"id in ('4000001')\"))#before row level dedup\n",
    "dedupdf1=scrubbeddf3.distinct()#It will remove the row level duplicates\n",
    "display(dedupdf1.where(\"id in ('4000001')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44abc9bd-d791-48e7-8d25-368c2813147c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17821181-8811-4741-8dfc-eb43957a6e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "#dedupdf1.coalesce(1).where(\"id in ('4000003')\").orderBy([\"id\",\"age\"],ascending=[True,False]).show(3)\n",
    "dedupdf2=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,False]).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c4fd8e8-68d3-4ec3-bbf3-983e5d68e920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleandf1.createOrReplaceTempView(\"cleandf1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d42ff7f-0ab4-4a51-9e6b-9f8c695cad4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW scrubbeddf1 AS\n",
    "SELECT\n",
    "  id,\n",
    "  firstname,\n",
    "  COALESCE(lastname, 'not provided')  AS lastname,\n",
    "  age,\n",
    "  COALESCE(profession, 'not provided') AS profession\n",
    "FROM cleandf1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566c4a4f-731f-4319-931b-9bbe48b81b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbeddf1.createOrReplaceTempView(\"scrubbeddf1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f615e0fe-9772-42f2-bb72-5ca1f215e63e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW scrubbeddf2 AS\n",
    "SELECT\n",
    "  id,\n",
    "  firstname,\n",
    "  lastname,\n",
    "  age,\n",
    "  CASE\n",
    "    WHEN profession = 'Pilot' THEN 'Captain'\n",
    "    WHEN profession = 'Actor' THEN 'Celeberity'\n",
    "    ELSE profession\n",
    "  END AS profession\n",
    "FROM scrubbeddf1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02edcec4-7fee-4b9f-a551-b71f0bb2d923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW dedupdf2_priority AS\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY id\n",
    "           ORDER BY id,age DESC\n",
    "         ) AS rn\n",
    "  FROM scrubbeddf2\n",
    ") t\n",
    "WHERE rn = 1;\n",
    "select * from dedupdf2_priority;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00176d7-6445-41a6-a019-8a73ab72ba1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "#withColumn(\"stringcolumnname to add in the df\",lit('hardcoded')/initcap(col(\"colname\")))\n",
    "standarddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"Retail\"))#SparkSQL - DSL(FBP)\n",
    "display(standarddf1.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea7f7b2-c5a8-4cbd-8176-b1e5bb24a662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW duplicateview AS\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY id ORDER BY id,age DESC) AS rn\n",
    "  FROM scrubbeddf2\n",
    ") t;\n",
    "--select * from duplicateview where rn=1;--deduplicated data\n",
    "select * from duplicateview qualify (ROW_NUMBER() OVER (PARTITION BY id ORDER BY id,age DESC))=1;\n",
    "\n",
    "-- select id, count(*)cnt from duplicateview group by id having cnt>1 order by id;--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc445a0-7d69-4734-805a-c9c22136e184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW standarddf1 AS\n",
    "SELECT\n",
    "  *,\n",
    "  'Retail' AS sourcesystem\n",
    "FROM dedupdf2_priority \n",
    "where upper(id) =lower(id);--wanted to take only number values in id column\n",
    "\n",
    "select * from standarddf1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d092568c-b817-429f-8acc-21a4ba3cc88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW standarddf2 AS\n",
    "SELECT\n",
    "  id,\n",
    "  firstname,\n",
    "  lastname,\n",
    "  age,\n",
    "  INITCAP(profession) AS profession,\n",
    "  sourcesystem\n",
    "FROM standarddf1;\n",
    "\n",
    "select * from standarddf2 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b4bac6-6967-4e08-b18e-e4b1a2b5201a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW standarddf3 AS\n",
    "SELECT\n",
    "  CASE\n",
    "    WHEN cast(id as string) = 'nine'  THEN '9'\n",
    "    WHEN cast(id as string) = 'ten'   THEN '10'\n",
    "    ELSE id\n",
    "  END AS id,\n",
    "  firstname,\n",
    "  lastname,\n",
    "  REGEXP_REPLACE(age, '-', '') AS age,\n",
    "  profession,\n",
    "  sourcesystem\n",
    "FROM standarddf2;\n",
    "\n",
    "select * from standarddf3 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6fe14c-9586-4319-ac15-af91c44f846e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view standarddf4 as \n",
    "SELECT \n",
    "    CAST(id AS BIGINT) AS id,\n",
    "    firstname,\n",
    "  lastname,\n",
    "  CAST(age AS SMALLINT) AS age,\n",
    "  profession,\n",
    "  sourcesystem\n",
    "FROM standarddf3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3290515e-2f4c-456f-be41-3296d4591f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view standarddf5 as\n",
    "select \n",
    "id custid,\n",
    "firstname,\n",
    "lastname,\n",
    "age,\n",
    "profession,\n",
    "sourcesystem as srcsystem\n",
    " from standarddf4;\n",
    "\n",
    " select * from standarddf5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba2a321-e5ad-4c12-bf90-fbeb25a01997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view mungeddf as \n",
    "SELECT \n",
    "    custid, \n",
    "    age, \n",
    "    firstname,\n",
    "    lastname,\n",
    "    profession,\n",
    "    srcsystem\n",
    "FROM standarddf5;\n",
    "\n",
    "SELECT * FROM mungeddf LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abf30d0-9182-497b-b398-ba3cb023aa01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_filename='custsmodified_25/30/12.csv'\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "spark.sql(f\"\"\"\n",
    "    create or replace temp view enrichdf1 as \n",
    "    SELECT \n",
    "        *,\n",
    "        '{derived_datadt}' AS datadt,\n",
    "        current_date() AS loaddt\n",
    "    FROM mungeddf\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3849e5b7-8177-4bbb-9fe1-b1bd80faae50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from enrichdf1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49bea1d7-b1a6-4085-9fc5-ec3aa6d0d63b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view enrichdf2 as \n",
    "SELECT \n",
    "    *,\n",
    "    substring(profession, 1, 1) AS professionflag\n",
    "FROM enrichdf1;\n",
    "\n",
    "SELECT * FROM enrichdf2 LIMIT 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1929911b-7249-4d0e-be04-39d637ddcb51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view enrichdf3 as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    firstname,\n",
    "    lastname,\n",
    "    profession,\n",
    "    srcsystem AS sourcename, -- Renaming occurs here\n",
    "    datadt,\n",
    "    loaddt,\n",
    "    professionflag AS profflag -- Renaming occurs here\n",
    "FROM enrichdf2;\n",
    "\n",
    "SELECT * FROM enrichdf3 LIMIT 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff13a98c-4551-4cf2-96e9-3ba670a2af8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view enrichdf4 as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    firstname,\n",
    "    lastname,\n",
    "    concat(profession, '-', profflag) AS profession, -- Overwrites/Modifies the profession column\n",
    "    sourcename,\n",
    "    datadt,\n",
    "    loaddt,\n",
    "    profflag\n",
    "FROM enrichdf3;\n",
    "\n",
    "SELECT * FROM enrichdf4 LIMIT 20;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4516843065977701,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
